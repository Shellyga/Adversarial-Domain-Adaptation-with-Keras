{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emb of Gan Working last chance Adversarial-Domain-Adaptation-with-Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shellyga/Adversarial-Domain-Adaptation-with-Keras/blob/master/emb_of_Gan_Working_last_chance_Adversarial_Domain_Adaptation_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqYXgaotqk_g",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57CQpjm7OklS",
        "colab_type": "code",
        "outputId": "e1fa80b1-868f-4521-a611-7057e800522a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "pip install keras_vggface"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_vggface\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/7d/5f0319ebdc09ac1a2272364fa9583f5067b6f8aff93fbbf8835d81cbaad7/keras_vggface-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.18.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.3.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (7.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.0.8)\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxcy4fLWqpgf",
        "colab_type": "code",
        "outputId": "744155e2-cb93-4b62-c95f-134fcba0f34e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense,Reshape\n",
        "from keras.layers import BatchNormalization, Activation, Dropout\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "\n",
        "# def build_embedding(param, inp):\n",
        "#     network = eval(param[\"network_name\"])\n",
        "#     # base = network(weights = 'vggface', include_top = False)\n",
        "#     base = network(weights = 'imagenet', include_top = False)\n",
        "#     feat = base(inp)\n",
        "#     print(feat.shape)\n",
        "#     flat = Flatten()(feat)\n",
        "#     return flat\n",
        "\n",
        "def build_embedding(param, inp):\n",
        "    network = eval('VGGFace')\n",
        "    base = network(weights = 'vggface', include_top = False)\n",
        "    # base = network(weights = 'imagenet', include_top = False)\n",
        "    feat = base(inp)\n",
        "    # return feat\n",
        "    print(feat.shape)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "# def build_embedding(param,inp):\n",
        "#     model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), weights='vggface', pooling='avg')\n",
        "#     feat = model.get_layer('avg_pool').output\n",
        "#     # return feat\n",
        "#     # print(feat.shape)\n",
        "#     flat = Flatten()(feat)\n",
        "#     # flat = Reshape((-1,))(feat)\n",
        "#     # flat = Flatten()(embedding)\n",
        "#     # print(flat.shape)\n",
        "#     return flat\n",
        "\n",
        "def build_classifier(param, img):\n",
        "    # # embedding = Input(embedding.shape)\n",
        "    # # embedding = Input( (None, 100352) )\n",
        "    embedding = Input( (None, 25088) )\n",
        "    # # embedding = Reshape((-1,))(embedding)\n",
        "    # # flat = Flatten()(embedding)\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "    densel = Dense(param[\"number_of_classe\"], name = 'class_dense_last')(drop2)\n",
        "    # densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return Model(input=embedding, outputs=actl)\n",
        "\n",
        "def build_discriminator(param, img):\n",
        "    # # embedding = Input(embedding.shape)\n",
        "    # # embedding = Input( (None, 100352) )\n",
        "    embedding = Input( (None, 25088) )\n",
        "    # # embedding = Reshape((-1,))(embedding)\n",
        "    # # flat = Flatten()(embedding)\n",
        "    dense1 = Dense(400, name = 'dis_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name='dis_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'dis_act1')(bn1)\n",
        "    drop1 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'dis_dense2')(drop1)\n",
        "    bn2 = BatchNormalization(name='dis_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'dis_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop2')(act2)\n",
        "\n",
        "    densel = Dense(1, name = 'dis_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'dis_bn_last')(densel)\n",
        "    actl = Activation('sigmoid', name = 'dis_act_last')(bnl)\n",
        "    return Model(input=embedding, outputs=actl)\n",
        "\n",
        "def build_combined_classifier(inp, classifier):\n",
        "    comb_model = Model(inputs = inp, outputs = [classifier])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_discriminator(inp, discriminator):\n",
        "    comb_model = Model(inputs = inp, outputs = [discriminator])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_model(inp, comb):\n",
        "    comb_model = Model(inputs = inp, outputs = comb)\n",
        "    return comb_model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-530853b0be5e>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    embedding = Input( (, 25088) )\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DrTIu1SqtIe",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8K7kUmqu5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def opt_classifier(param):\n",
        "    return Adam(lr=param[\"lr_classifier\"], beta_1=param[\"b1_classifier\"], beta_2=param[\"b2_classifier\"])\n",
        "\n",
        "def opt_discriminator(param):\n",
        "    return Adam(lr=param[\"lr_discriminator\"], beta_1=param[\"b1_discriminator\"], beta_2=param[\"b2_discriminator\"])\n",
        "\n",
        "def opt_combined(param):\n",
        "    return Adam(lr=param[\"lr_combined\"], beta_1=param[\"b1_combined\"], beta_2=param[\"b2_combined\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHRxvLjYqwvG",
        "colab_type": "text"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZPFQrHKAotL",
        "colab_type": "text"
      },
      "source": [
        "# Changes I made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlgU5w6qyJO",
        "colab_type": "code",
        "outputId": "3a497bab-27ef-4de6-9c20-0a2ce89c19e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwuSxj4hW9vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 7\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "# import tensorflow.python.keras as tf\n",
        "from tensorflow.compat.v1 import set_random_seed\n",
        "# import tensorflow.python.keras as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from plotnine import *\n",
        "import pandas as pd\n",
        "                \n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "from sklearn.utils import shuffle as skshuffle\n",
        "\n",
        "from imutils import paths\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # print(path)\n",
        "    # Return the RGB variant of input image\n",
        "    with open(path, 'rb') as f:\n",
        "      with Image.open(f) as img:\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def one_hot_encoding(param):\n",
        "    lb = LabelEncoder()\n",
        "    source_labels,target_labels  = lb.fit_transform( param[\"source_label\"]),lb.fit_transform( param[\"target_label\"])\n",
        "    param[\"number_of_classe\"] = len(lb.classes_)\n",
        "    source_labels,target_labels = to_categorical(source_labels),to_categorical(target_labels)\n",
        "    return source_labels,target_labels\n",
        "    # # Read the source and target labels from param\n",
        "    # s_label = param[\"source_label\"]\n",
        "    # t_label = param[\"target_label\"]\n",
        "\n",
        "    # # Encode the labels into one-hot format\n",
        "    # classes = (np.concatenate((s_label, t_label), axis = 0))\n",
        "    # num_classes = np.max(classes)\n",
        "    # if 0 in classes:\n",
        "    #         num_classes = num_classes+1\n",
        "    # s_label = to_categorical(s_label, num_classes = num_classes)                \n",
        "    # t_label = to_categorical(t_label, num_classes = num_classes)\n",
        "    # return s_label, t_label\n",
        "            \n",
        "def data_loader(filepath, inp_dims):\n",
        "    # Load images and corresponding labels from the text file, stack them in numpy arrays and return\n",
        "    # if not os.path.isfile(filepath):\n",
        "    #     print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
        "        # sys.exit() \n",
        "    img = []\n",
        "    label = []\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = \"drive/My Drive/final_proj_dataset/shape_predictor_5_face_landmarks.dat\"\n",
        "    # with open(filepath,'r',encoding='utf-8-sig') as fp:\n",
        "    #     for line in fp:\n",
        "    #         token = line.split()\n",
        "    #         image_path = \"drive/My Drive/final_proj_dataset/\"+token[0]\n",
        "    # grab the image paths\n",
        "    imagePaths = sorted(list(paths.list_images(filepath)))\n",
        "    print(imagePaths)\n",
        "    # loop over the input images\n",
        "    for imagePath in imagePaths:\n",
        "        # extract the class label from the image path and update the\n",
        "        # labels list\n",
        "        l = imagePath.split(os.path.sep)[-2]\n",
        "        label.append(l)\n",
        "        i = pil_loader(imagePath)\n",
        "        # i = cv2.imread(imagePath)\n",
        "        i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "        frame_1 = align_and_crop(detector, np.array(i), predictor)[0]\n",
        "        frame_1 = cv2.resize(frame_1,(inp_dims[0], inp_dims[1]))\n",
        "        img.append(frame_1)\n",
        "        # label.append(int(token[1]))\n",
        "    # img, label = skshuffle(img, label)\n",
        "    img = np.array(img)\n",
        "    label = np.array(label)\n",
        "    return img, label\n",
        "    \n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    #Generate batches of data.\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE5GvMe6AsOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp) \n",
        "    # embedding = build_embedding(param)   \n",
        "    \n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_discriminator(param, img)\n",
        "    discriminator.compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # Build the generator\n",
        "    generator = Model(inp,embedding)\n",
        "\n",
        "    # The generator takes noise as input and generates imgs\n",
        "    z = Input(shape=(param[\"inp_dims\"]))\n",
        "    rep = generator(z)\n",
        "\n",
        "    # For the combined model we will only train the generator\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # The discriminator takes generated images as input and determines validity\n",
        "    valid = discriminator(rep)\n",
        "\n",
        "    # The combined model  (stacked generator and discriminator)\n",
        "    # Trains the generator to fool the discriminator\n",
        "    combined = Model(z, valid)\n",
        "       models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = ['categorical_crossentropy', 'binary_crossentropy'] , loss_weights =  [param[\"class_loss_weight\"],  param[\"dis_loss_weight\"]], metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "\n",
        "    # The discriminator takes the representaton as input and determines the domain\n",
        "    models[\"combined_discriminator\"] = Model(inputs=inp, outputs=discriminator(embedding))\n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    # For the combined model we will only train the generator\n",
        "    discriminator.trainable = False\n",
        "  \n",
        "    # Build the classifier\n",
        "    classifier = build_classifier(param, embedding)\n",
        "    models['combined_classifier'] = Model(inputs=inp, outputs=classifier(embedding))\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    models[\"combined_model\"] = build_combined_model(inp, [classifier(embedding), discriminator(embedding)]) \n",
        "    # models[\"combined_model\"] = build_combined_model(inp, [freezed_classifier(embedding), discriminator(embedding)]) \n",
        "    models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = ['categorical_crossentropy', 'binary_crossentropy'] , loss_weights =  [param[\"class_loss_weight\"],  param[\"dis_loss_weight\"]], metrics = ['accuracy'])\n",
        "\n",
        "    models[\"combined_classifier\"].summary()\n",
        "    models[\"combined_discriminator\"].summary()\n",
        "    models[\"combined_model\"].summary()\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt, Xt_test, yt, yt_test = train_test_split(Xt, yt, test_size=0.4, random_state=42)\n",
        "    Xs, Xs_test, ys, ys_test = train_test_split(Xs, ys, test_size=0.4, random_state=42)\n",
        "    \n",
        "    # Xs_test, ys_test = param[\"source_data_test\"], param[\"source_label_test\"]\n",
        "    # Xt_test, yt_test = param[\"target_data_test\"], param[\"target_label_test\"]\n",
        "    Xs, ys = aug_training_set_loader(Xs, ys, param[\"inp_dims\"])\n",
        "    Xt, yt = aug_training_set_loader(Xt, yt, param[\"inp_dims\"])\n",
        "\n",
        "    Xs_test, ys_test = skshuffle( Xs_test, ys_test)\n",
        "    Xt_test, yt_test = skshuffle( Xt_test, yt_test)\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "    loss_discriminator = []\n",
        "    loss_classifier_source=[]\n",
        "\n",
        "    t_domain_label = np.ones(( param[\"batch_size\"], 1))\n",
        "    s_domain_label = np.zeros(( param[\"batch_size\"], 1))\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "        # print(X_adv.shape)\n",
        "        plt.imshow(X_adv[0])\n",
        "        plt.show()\n",
        "        # Extract a batch of new features????????????\n",
        "        # s_features = models[\"combined_discriminator\"].predict(Xsb)\n",
        "        # t_features = models[\"combined_discriminator\"].predict(Xtb)\n",
        "\n",
        "        # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        # d_loss_s = models[\"combined_discriminator\"].train_on_batch(Xsb, s_domain_label)\n",
        "        # d_loss_t = models[\"combined_discriminator\"].train_on_batch(Xtb, t_domain_label)\n",
        "        # d_loss = 0.5 * np.add(d_loss_s, d_loss_t)\n",
        "        if ((i+1 ) % 2 == 0):\n",
        "          d_loss = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images domains)\n",
        "        g_loss = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1])\n",
        "\n",
        "        # # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        # d_loss = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "        # # d_loss_source = models[\"combined_discriminator\"].train_on_batch(Xsb, [ysb])\n",
        "        # # d_loss_target = models[\"combined_discriminator\"].train_on_batch(Xtb, ytb)\n",
        "        # # d_loss = 0.5 * np.add(d_loss_source, d_loss_target)\n",
        "\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt)\n",
        "            \n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "            acc_domain_source.append(source_domain_accuracy)\n",
        "            acc_domain_target.append(target_domain_accuracy)\n",
        "\n",
        "            loss_discriminator.append(d_loss[1])\n",
        "            loss_classifier_source.append(g_loss[1])\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                optim[\"labels\"] = ys_pred.argmax(1)\n",
        "                param[\"target_accuracy\"] = target_accuracy\n",
        "\n",
        "        #         if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "        #             gap_last_snap = 0\n",
        "        #             np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "        #             open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "        #         #     models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        # gap_last_snap = gap_last_snap + 1;\n",
        "    \n",
        "    print(optim[\"iter\"],optim[\"acc\"],optim[\"labels\"])\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "    # ys_test_adv_pred = models[\"combined_discriminator\"].predict(Xs_test)\n",
        "    # yt_test_adv_pred = models[\"combined_discriminator\"].predict(Xt_test)\n",
        "    # domain_source_accuracy_test = accuracy_score(ys_adv, np.round(ys_test_adv_pred))              \n",
        "    # domain_target_accuracy_test = accuracy_score(yt_adv, np.round(yt_test_adv_pred))\n",
        "\n",
        "    # print(\"discriminator source accuracy test\",domain_source_accuracy_test)\n",
        "    # print(\"discriminator target accuracy test\",domain_target_accuracy_test)\n",
        "\n",
        "    # print(param[\"num_iterations\"]/ (param[\"test_interval\"]-1))\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_source), label=\"Domain Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_target), label=\"Domain Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(loss_classifier_source), label=\"Classifier Loss\")\n",
        "    plt.plot(N, np.array(loss_discriminator), label=\"Discriminator loss\")\n",
        "    plt.title(\"Training Loss of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-cnKs4F7_fO",
        "colab_type": "text"
      },
      "source": [
        "# New train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBwDR_lc0WSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "def train(param):\n",
        "    models = {}\n",
        "    # Build classifier\n",
        "    model_path = 'drive/My Drive/final_proj_dataset/result2/model.h5'\n",
        "    # load model\n",
        "    classifier = load_model(model_path)\n",
        "    # summarize model.\n",
        "    classifier.summary()\n",
        "\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp) \n",
        "    # embedding = build_embedding(param)  \n",
        "\n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_discriminator(param,embedding )\n",
        "    discriminator.summary()\n",
        "    discriminator.compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # Build the encoder \n",
        "    encoder = Model(inp,embedding)\n",
        "\n",
        "    img = Input(shape=param[\"inp_dims\"])\n",
        "    # The generator takes the image, and encodes it\n",
        "    encoded_repr = encoder(img)\n",
        "\n",
        "    # For the adversarial_autoencoder model we will only train the generator\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # The discriminator determines the domain of the encoding\n",
        "    domain = discriminator(encoded_repr)\n",
        "\n",
        "    # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
        "    adversarial_autoencoder = Model(img, [encoded_repr, domain])\n",
        "    adversarial_autoencoder.summary()\n",
        "    adversarial_autoencoder.compile(optimizer = opt_combined(param), loss = ['categorical_crossentropy', 'binary_crossentropy'] , loss_weights =  [param[\"class_loss_weight\"],  param[\"dis_loss_weight\"]], metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt, Xt_test, yt, yt_test = train_test_split(Xt, yt, test_size=0.4, random_state=42)\n",
        "    Xs, Xs_test, ys, ys_test = train_test_split(Xs, ys, test_size=0.4, random_state=42)\n",
        "\n",
        "    Xs, ys = aug_training_set_loader(Xs, ys, param[\"inp_dims\"])\n",
        "    Xt, yt = aug_training_set_loader(Xt, yt, param[\"inp_dims\"])\n",
        "\n",
        "    Xs_test, ys_test = skshuffle( Xs_test, ys_test)\n",
        "    Xt_test, yt_test = skshuffle( Xt_test, yt_test)\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "    loss_discriminator = []\n",
        "    loss_classifier_source=[]\n",
        "\n",
        "    t_domain_label = np.ones(( param[\"batch_size\"], 1))\n",
        "    s_domain_label = np.zeros(( param[\"batch_size\"], 1))\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "        rep_s = encoder.predict(Xsb)\n",
        "        rep_t = encoder.predict(Xtb)\n",
        "        print(rep_s.shape)\n",
        "        rep_s = np.expand_dims(rep_s, axis=0)\n",
        "        print(rep_s.shape)\n",
        "        rep_t = np.expand_dims(rep_t, axis=0)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_source = discriminator.train_on_batch(rep_s, s_domain_label)\n",
        "        d_loss_targrt = discriminator.train_on_batch(rep_t, t_domain_label)\n",
        "        d_loss = 0.5 * np.add(d_loss_source, d_loss_targrt)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator\n",
        "        g_loss = adversarial_autoencoder.train_on_batch(X_adv, [X_adv, y_advb_1])\n",
        "\n",
        "        # Plot the progress\n",
        "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
        "\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            pred_rep_s = encoder.predict(Xsb)\n",
        "            pred_rep_t = encoder.predict(Xtb)\n",
        "            ys_pred = classifier.predict(pred_rep_s)\n",
        "            yt_pred = classifier.predict(pred_rep_t)\n",
        "            \n",
        "            ys_adv_pred = discriminator.predict(pred_rep_s)\n",
        "            yt_adv_pred = discriminator.predict(pred_rep_t)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "            acc_domain_source.append(source_domain_accuracy)\n",
        "            acc_domain_target.append(target_domain_accuracy)\n",
        "\n",
        "            loss_discriminator.append(d_loss[1])\n",
        "            loss_classifier_source.append(g_loss[1])\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                optim[\"labels\"] = ys_pred.argmax(1)\n",
        "                param[\"target_accuracy\"] = target_accuracy\n",
        "\n",
        "        #         if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "        #             gap_last_snap = 0\n",
        "        #             np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "        #             open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "        #         #     models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        # gap_last_snap = gap_last_snap + 1;\n",
        "    \n",
        "    print(optim[\"iter\"],optim[\"acc\"],optim[\"labels\"])\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_source), label=\"Domain Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_target), label=\"Domain Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(loss_classifier_source), label=\"Classifier Loss\")\n",
        "    plt.plot(N, np.array(loss_discriminator), label=\"Discriminator loss\")\n",
        "    plt.title(\"Training Loss of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdnIND2JEHUn",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq_tQgLEEETm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "if __name__ == \"__main__\":\n",
        "    # Read parameter values from the console\n",
        "    parser = argparse.ArgumentParser(description = 'Domain Adaptation')\n",
        "    parser.add_argument('--number_of_gpus', type = int, nargs = '?', default = '1', help = \"Number of gpus to run\")\n",
        "    parser.add_argument('--network_name', type = str, default = 'ResNet50', help = \"Name of the feature extractor network\")\n",
        "    parser.add_argument('--dataset_name', type = str, default = 'Office', help = \"Name of the source dataset\")\n",
        "    parser.add_argument('--dropout_classifier', type = float, default = 0.25, help = \"Dropout ratio for classifier\")\n",
        "    parser.add_argument('--dropout_discriminator', type = float, default = 0.25, help = \"Dropout ratio for discriminator\")    \n",
        "    parser.add_argument('--source_path', type = str, default = 'amazon_10_list.txt', help = \"Path to source dataset\")\n",
        "    parser.add_argument('--target_path', type = str, default = 'webcam_10_list.txt', help = \"Path to target dataset\")\n",
        "    parser.add_argument('--lr_classifier', type = float, default = 0.0001, help = \"Learning rate for classifier model\")\n",
        "    parser.add_argument('--b1_classifier', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for classifier model optimizer\")\n",
        "    parser.add_argument('--b2_classifier', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for classifier model optimizer\")\n",
        "    parser.add_argument('--lr_discriminator', type = float, default = 0.00001, help = \"Learning rate for discriminator model\")\n",
        "    parser.add_argument('--b1_discriminator', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for discriminator model optimizer\")\n",
        "    parser.add_argument('--b2_discriminator', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for discriminator model optimizer\")\n",
        "    parser.add_argument('--lr_combined', type = float, default = 0.00001, help = \"Learning rate for combined model\")\n",
        "    parser.add_argument('--b1_combined', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for combined model optimizer\")\n",
        "    parser.add_argument('--b2_combined', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for combined model optimizer\")\n",
        "    parser.add_argument('--classifier_loss_weight', type = float, default = 1, help = \"Classifier loss weight\")\n",
        "    parser.add_argument('--discriminator_loss_weight', type = float, default = 4, help = \"Discriminator loss weight\")\n",
        "    parser.add_argument('--batch_size', type = int, default = 32, help = \"Batch size for training\")\n",
        "    parser.add_argument('--test_interval', type = int, default = 3, help = \"Gap between two successive test phases\")\n",
        "    parser.add_argument('--num_iterations', type = int, default = 120, help = \"Number of iterations\")\n",
        "    parser.add_argument('--snapshot_interval', type = int, default = 500, help = \"Minimum gap between saving outputs\")\n",
        "    parser.add_argument('--output_dir', type = str, default = 'Models', help = \"Directory for saving outputs\")\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Set GPU device\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(list(np.arange(args.number_of_gpus))).strip('[]')\n",
        "\n",
        "    # Initialize parameters\n",
        "    param = {}\n",
        "    param[\"number_of_gpus\"] = 1\n",
        "    # param[\"network_name\"] = 'ResNet50'\n",
        "    param[\"network_name\"] = 'VGGFace'\n",
        "    param[\"inp_dims\"] = [224, 224, 3]\n",
        "    param[\"num_iterations\"] = 2\n",
        "    # param[\"num_iterations\"] = 500\n",
        "    param[\"lr_classifier\"] = 0.0001\n",
        "    param[\"b1_classifier\"] = 0.9\n",
        "    param[\"b2_classifier\"] = 0.999    \n",
        "    param[\"lr_discriminator\"] = 0.00001\n",
        "    param[\"b1_discriminator\"] =  0.9\n",
        "    param[\"b2_discriminator\"] = 0.999\n",
        "    param[\"lr_combined\"] = 0.00001\n",
        "    param[\"b1_combined\"] =  0.9\n",
        "    param[\"b2_combined\"] =  0.999       \n",
        "    # param[\"batch_size\"] = int(32)\n",
        "    param[\"batch_size\"] = int(32/2)\n",
        "    param[\"class_loss_weight\"] = 1\n",
        "    param[\"dis_loss_weight\"] = 4    \n",
        "    param[\"drop_classifier\"] = 0.25\n",
        "    param[\"drop_discriminator\"] = 0.25\n",
        "    param[\"test_interval\"] = 1\n",
        "    # param[\"source_path\"] = 'drive/My Drive/final_proj_dataset/data_file_shelly.txt'\n",
        "    # param[\"target_path\"] = 'drive/My Drive/final_proj_dataset/data_file_yerus.txt' \n",
        "    param[\"source_path\"] = 'drive/My Drive/pro_data/dataset_shelly'\n",
        "    param[\"target_path\"] = 'drive/My Drive/pro_data/dataset_yerus' \n",
        "    param[\"snapshot_interval\"] = 500\n",
        "    # param[\"snapshot_interval\"] = 5\n",
        "    param[\"output_path\"] = 'drive/My Drive/final_proj_dataset/result2'\n",
        "    param[\"number_of_classe\"] = 0\n",
        "\n",
        "    # # Create directory for saving models and log files\n",
        "    if not os.path.exists(param[\"output_path\"]):\n",
        "        os.mkdir(param[\"output_path\"])\n",
        "    \n",
        "    # print(\"[INFO] loading images...\")\n",
        "    # # Load source and target data\n",
        "    # param[\"source_data\"], param[\"source_label\"] = data_loader(param[\"source_path\"], param[\"inp_dims\"])\n",
        "    # param[\"target_data\"], param[\"target_label\"] = data_loader(param[\"target_path\"], param[\"inp_dims\"])\n",
        "    # # param[\"source_data\"], param[\"source_label\"],param[\"target_data\"], param[\"target_label\"] =  load_data(param[\"source_path\"],param[\"target_path\"])\n",
        "\n",
        "    # # Encode labels into one-hot format\n",
        "    # param[\"source_label\"], param[\"target_label\"] = one_hot_encoding(param)\n",
        "\n",
        "    \n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42msC3B1fJoH",
        "colab_type": "code",
        "outputId": "6810823d-085e-4db7-f4fa-1de1a7841c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "print(\"[INFO] loading images...\")\n",
        "# Load source and target data\n",
        "param[\"source_data\"], param[\"source_label\"] = data_loader(param[\"source_path\"], param[\"inp_dims\"])\n",
        "print(\"source images loaded\")\n",
        "param[\"target_data\"], param[\"target_label\"] = data_loader(param[\"target_path\"], param[\"inp_dims\"])\n",
        "print(\"target images loaded\")\n",
        "# Encode labels into one-hot format\n",
        "print(\"[INFO] Encode labels into one-hot format\")\n",
        "param[\"source_label\"], param[\"target_label\"] = one_hot_encoding(param)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "['drive/My Drive/pro_data/dataset_shelly/no/20150910_095805.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184612.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184615.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184618.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184622.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184626.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20150913_184904.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151011_152558.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151011_152642.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151018_212830.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151024_113302.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151111_195411.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151203_144530.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20151203_144604.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160422_221701.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160515_202052.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160515_202226.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160515_202253.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160908_043514.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160908_181317.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160909_212542.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160909_212555.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160912_114911.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20160912_140137_HDR.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161007_133123.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161007_133159.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161007_134106.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161008_172904.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161008_172910.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161020_150314.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161031_132016.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161031_141125.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20161107_231938.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170111_123505.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170129_170914.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170305_075514.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170305_080044.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170312_112035.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170323_153005.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170324_235042.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170328_175715.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170328_175720.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170828_062844.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170828_165702.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170904_120613.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170919_125315.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170920_221144.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170921_001830.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170921_121622(1).jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20170921_121622.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171002_213252.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171002_213308.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171007_030952.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171007_031022.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171007_031029.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171010_150339.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171010_150343.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171020_234719.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171027_223923.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171027_223936.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171027_223943.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171031_114429.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171031_114519.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171101_182918.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171103_141527.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171103_214818.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171103_214844.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171103_220131.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20171110_032103.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20180319_113601.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20180319_113604.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20180408_010945.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20180924_131652.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20181014_140330.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20181014_141053.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190422_164538.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190422_164542.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190426_124034.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190426_124048.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190615_124511.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20190928_114131.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191004_134157.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191004_134159.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191004_134323.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191004_134325.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191004_153738.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191012_210430.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191012_210431.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191012_210446.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191014_233217.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191016_105820.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191016_110125.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191017_145943.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191026_141948.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191102_233359.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191102_233441.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191102_233443.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191112_130437.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191130_130050.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191207_144014.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20191218_105157.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200213_101915_001.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200215_121016.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200304_113613.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_150720.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_150747.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_150749.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_150846.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_151003.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_152450.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200305_152553.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200307_121924.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200310_104029.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200310_104554.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200310_220902.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/20200310_220928.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160115-WA0032.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160120-WA0009.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160212-WA0020.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160423-WA0020.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160511-WA0003.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160511-WA0005.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160527-WA0001.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20160813-WA0000.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20181102-WA0030.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG-20190321-WA0000.jpg', 'drive/My Drive/pro_data/dataset_shelly/no/IMG_20170621_140020_659.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190420_125159.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190420_135001.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190420_141636.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190420_141646.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190424_182110.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190424_182119.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190426_124338.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190426_142042.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190427_123558.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190503_151627.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190509_124048.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190509_124051.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190517_174424.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190609_145436.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190609_145448.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190615_132303.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190615_132306.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190622_193342.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190703_213549.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190703_213552.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190703_215737.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190703_215740.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190713_194146.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190713_194149.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190713_194307.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190831_105716.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190905_011747.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190914_114144.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190914_114149.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190921_131355.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190928_114110.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190928_114121.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20190928_122856.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191001_133552.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191004_134202.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191004_134328.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191006_172735.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191006_181230.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191010_130801.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191012_123004.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191012_123008.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191012_210434.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191012_210457.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191012_210500.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191016_110127.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191019_135718.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191019_135732.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191019_135741.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191112_130439.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191112_130444.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191116_151517.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191116_151520.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191207_143737.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191207_143743.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191207_144035.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20191207_144037.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200115_121438.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200215_120911.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200304_113618.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200304_113620.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200304_113659.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150841.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150859.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150901.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150935.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150945.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150947.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150948.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_150950.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_151005.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_151007.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_151008.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_151035.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_151036.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200305_152547.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200310_104043.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200310_104044.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200310_104548.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200310_104550.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200311_120236.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200311_120458.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200311_120459.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200311_120736.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200311_120737.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200314_121652.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200314_121654.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200314_121710.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200314_121718.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200314_121727.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200331_190417.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200331_190423.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200331_190427.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200331_190453.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200331_190454.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_185954.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_185955.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_185958.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_190659.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_190703.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_212407.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_212414.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_212445.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200408_212447.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200411_120622.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200411_120624.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202043.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202045.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202050.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202054.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202055.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202100.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202105.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202107.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202203.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202205.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202209.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202217.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202221.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202223.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202224.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202233.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/20200428_202236.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/FB_IMG_1576537318259.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/FB_IMG_1576537426145.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/FB_IMG_1576537431025.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/FB_IMG_1576537434281.jpg', 'drive/My Drive/pro_data/dataset_shelly/yes/FB_IMG_1576537436719.jpg']\n",
            "source images loaded\n",
            "['drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-104.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1041.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1048.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1083.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1102.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1146.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1168.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1169.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1170.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1225.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1260.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-128.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1301.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1302.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1303.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1305.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-134.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-135.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1360.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1366.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1390.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1391.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1426.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1432.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1448.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1467.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1489.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1512.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1522.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1575.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1580.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1586.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-160.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1608.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1609.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1623.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1645.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1646.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1708.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1710.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1732.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1733.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1743.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1795.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1796.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1806.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1832.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1843.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1909.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1927.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1928.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1929.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1930.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1952.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-1971.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2006.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2007.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2051.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2059.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2085.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-21.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2128.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2129.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-213.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2130.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-214.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2149.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2150.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-216.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2191.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2192.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2227.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2369.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2370.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2371.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2379.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2448.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2449.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2458.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2492.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2534.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2535.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2537.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2543.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2599.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2621.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-2622.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-292.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-302.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-337.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-354.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-401.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-43.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-434.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-435.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-436.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-476.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-512.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-534.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-535.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-564.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-61.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-621.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-622.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-642.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-664.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-686.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-70.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-700.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-744.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-754.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-755.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-821.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-827.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-828.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-842.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-882.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-918.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-949.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-970.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-975.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS-985.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS110.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS28.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS61.jpg', 'drive/My Drive/pro_data/dataset_yerus/no/IMG_N_YERUS62.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-103.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-106.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1063.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1064.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1077.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1082.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1083.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1110.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1115.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1116.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1117.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1118.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1304.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1309.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1318.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1319.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1320.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1451.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1456.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1457.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1462.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1475.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1498.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1499.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1500.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1509.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1510.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1533.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1534.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1543.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1552.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1553.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-161.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1628.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1629.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-163.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1630.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1631.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-164.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1640.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1730.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1731.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1744.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1749.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1750.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1777.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1782.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1783.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1784.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1785.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1816.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1829.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1830.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-184.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1864.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1865.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1870.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1883.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1906.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1915.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1916.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1950.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1984.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-1997.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2002.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2025.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2026.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2035.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2036.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2037.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2079.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2084.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2107.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2112.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2113.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2118.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-2123.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-315.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-318.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-350.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-351.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-369.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-370.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-402.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-403.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-404.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-405.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-457.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-466.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-467.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-5.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-523.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-533.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-534.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-535.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-646.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-659.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-664.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-665.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-670.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-697.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-698.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-699.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-752.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-757.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-79.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-80.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-823.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-824.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-829.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-856.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-857.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-862.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-875.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-876.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-940.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-941.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-942.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-943.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS-944.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS106.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS107.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS49.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS5.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS80.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS81.jpg', 'drive/My Drive/pro_data/dataset_yerus/yes/IMG_SMILE_YERUS97.jpg']\n",
            "target images loaded\n",
            "[INFO] Encode labels into one-hot format\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vaKAXuoj3A",
        "colab_type": "code",
        "outputId": "10d30077-a63b-4563-c0a0-3cef20220428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train data\n",
        "print(\"[INFO] training network...\")\n",
        "train(param)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        (None, None, 25088)       0         \n",
            "_________________________________________________________________\n",
            "class_dense1 (Dense)         (None, None, 400)         10035600  \n",
            "_________________________________________________________________\n",
            "class_bn1 (BatchNormalizatio (None, None, 400)         1600      \n",
            "_________________________________________________________________\n",
            "class_act1 (Activation)      (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "class_drop1 (Dropout)        (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "class_dense2 (Dense)         (None, None, 100)         40100     \n",
            "_________________________________________________________________\n",
            "class_bn2 (BatchNormalizatio (None, None, 100)         400       \n",
            "_________________________________________________________________\n",
            "class_act2 (Activation)      (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "class_drop2 (Dropout)        (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "class_dense_last (Dense)     (None, None, 2)           202       \n",
            "_________________________________________________________________\n",
            "class_bn_last (BatchNormaliz (None, None, 2)           8         \n",
            "_________________________________________________________________\n",
            "class_act_last (Activation)  (None, None, 2)           0         \n",
            "=================================================================\n",
            "Total params: 10,077,910\n",
            "Trainable params: 10,076,906\n",
            "Non-trainable params: 1,004\n",
            "_________________________________________________________________\n",
            "(None, 7, 7, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"di..., inputs=Tensor(\"in...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_53 (InputLayer)        (None, None, 25088)       0         \n",
            "_________________________________________________________________\n",
            "dis_dense1 (Dense)           (None, None, 400)         10035600  \n",
            "_________________________________________________________________\n",
            "dis_bn1 (BatchNormalization) (None, None, 400)         1600      \n",
            "_________________________________________________________________\n",
            "dis_act1 (Activation)        (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "dis_drop1 (Dropout)          (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "dis_dense2 (Dense)           (None, None, 100)         40100     \n",
            "_________________________________________________________________\n",
            "dis_bn2 (BatchNormalization) (None, None, 100)         400       \n",
            "_________________________________________________________________\n",
            "dis_act2 (Activation)        (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "dis_drop2 (Dropout)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "dis_dense_last (Dense)       (None, None, 1)           101       \n",
            "_________________________________________________________________\n",
            "dis_bn_last (BatchNormalizat (None, None, 1)           4         \n",
            "_________________________________________________________________\n",
            "dis_act_last (Activation)    (None, None, 1)           0         \n",
            "=================================================================\n",
            "Total params: 10,077,805\n",
            "Trainable params: 10,076,803\n",
            "Non-trainable params: 1,002\n",
            "_________________________________________________________________\n",
            "Model: \"model_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_54 (InputLayer)        (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "model_35 (Model)             (None, 25088)             14714688  \n",
            "_________________________________________________________________\n",
            "model_34 (Model)             multiple                  10077805  \n",
            "=================================================================\n",
            "Total params: 24,792,493\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 10,077,805\n",
            "_________________________________________________________________\n",
            "(16, 25088)\n",
            "(1, 16, 25088)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-5c28df238312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-39827fff49c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0md_loss_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_domain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0md_loss_targrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_domain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_targrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dis_act_last to have 3 dimensions, but got array with shape (16, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-om4DDXQRa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCAxthiKjwnr",
        "colab_type": "text"
      },
      "source": [
        "# Augment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD18oqysj0hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "from imgaug import seed\n",
        "def aug_training_set_loader(images,labels,inp_dims):\n",
        "    NUM_COPIES = ceil(1000/len(images))\n",
        "    images = np.array(images)\n",
        "    images=augment(inp_dims[0], inp_dims[1], images, NUM_COPIES)\n",
        "    aug_labels =[]\n",
        "    for x in labels:\n",
        "        for i in range(NUM_COPIES+1):  # NUM_COPIES+1-> numbers of augmentations + 1 original\n",
        "            aug_labels.append(x)\n",
        "    images, labels = skshuffle(images, aug_labels)\n",
        "    # images = np.array(images)\n",
        "    images = np.array(images, dtype=\"float\") / 255.0\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "\n",
        "def augment(width, height, data, NUM_COPIES):\n",
        "    \"\"\"\n",
        "    preform augmentetion on the list of photos named 'data'\n",
        "    :param width: width of a single photo\n",
        "    :param height: height of a single photo\n",
        "    :param data: a list of photos to augment\n",
        "    :param NUM_COPIES: number of copies produced from the image\n",
        "    :return: a list of photos that consists from the original photos and their augmentations\n",
        "    \"\"\"\n",
        "    augmented_data = []\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "    seq = iaa.Sequential([\n",
        "        sometimes(iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))),\n",
        "        sometimes(iaa.Fliplr()),  # horizontal flips\n",
        "        sometimes(iaa.AddElementwise((-50, 50))),\n",
        "        sometimes(iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0.0, 2.0)))),\n",
        "        sometimes(iaa.ContrastNormalization((0.8, 1.2))),\n",
        "        sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n",
        "        sometimes(iaa.Affine(\n",
        "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "            shear=(-8, 8)\n",
        "        )),\n",
        "        sometimes(iaa.Superpixels(p_replace=0.1, n_segments=150))\n",
        "    ], random_order=True)  # apply augmenters in random order\n",
        "    for img in data:\n",
        "        copies = augment_image(width, height, img, seq, NUM_COPIES)\n",
        "        copies.append(img.reshape(width, height, 3))\n",
        "        for cpy in copies:\n",
        "            augmented_data.append(cpy)\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "def augment_image(width, height, image, seq, NUM_COPIES):\n",
        "    \"\"\"\n",
        "        augments a single image\n",
        "    :param width: width of the image\n",
        "    :param height: height of the image\n",
        "    :param image: a matrix representing a rgb image\n",
        "    :param seq: the augmantation sequence preformed\n",
        "    :param NUM_COPIES: number of copies produced from the image\n",
        "    :return: a list of all images made from 'image' not including the original\n",
        "    \"\"\"\n",
        "    seed (1)\n",
        "    copies = []\n",
        "    image = image.reshape(width, height, 3)\n",
        "    for i in range(NUM_COPIES):\n",
        "        copies.append(seq.augment_image(image))\n",
        "\n",
        "    return copies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqiFjCePj2a1",
        "colab_type": "text"
      },
      "source": [
        "# Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSCoB73kj6ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# def resource_path(filename):\n",
        "#     \"\"\" Get absolute path to resource, for the executable\"\"\"\n",
        "#     if getattr(sys, 'frozen', False):\n",
        "#         # The application is frozen\n",
        "#         datadir = os.path.dirname(sys.executable)\n",
        "#     else:\n",
        "#         # The application is not frozen\n",
        "#         # Change this bit to match where you store your data files:\n",
        "#         datadir = os.path.dirname(__file__)\n",
        "\n",
        "    # return os.path.join(datadir, filename)\n",
        "\n",
        "def align_and_crop(detector, im_to_align, predictor_path):\n",
        "    '''\n",
        "    simply aligns the photo (using the detector to identify the eyes) and crops the face.\n",
        "    :param im_to_align: an address of an image\n",
        "    '''\n",
        "\n",
        "    # a shape predictor to find face landmarks so we can precisely localize the face\n",
        "    sp = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
        "    # second argument indicates that we should upsample the image 1 time.\n",
        "    try:\n",
        "        dets = detector(im_to_align, 1)\n",
        "    except RuntimeError:\n",
        "        return -1\n",
        "\n",
        "    num_faces = len(dets)\n",
        "    if num_faces == 0:\n",
        "        return im_to_align, False\n",
        "\n",
        "    # Find the 5 face landmarks we need to do the alignment.\n",
        "    faces = dlib.full_object_detections()\n",
        "    for detection in dets:\n",
        "        faces.append(sp(im_to_align, detection))\n",
        "\n",
        "    # get a single chip (aligned and cropped)\n",
        "    image = dlib.get_face_chip(im_to_align, faces[0])\n",
        "    # cv2.imshow(\"f\", image) show image for testing\n",
        "    return image, True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA7HFj4Rzgj0",
        "colab_type": "text"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EITQL6jazlp8",
        "colab_type": "code",
        "outputId": "7afec2b3-f923-4208-c06a-5aabe0cf6016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "\n",
        "model_path = 'drive/My Drive/final_proj_dataset/result2/model.h5'\n",
        "# load model\n",
        "model = load_model(model_path)\n",
        "# summarize model.\n",
        "model.summary()\n",
        "# load dataset\n",
        "\n",
        "score_Target = model.evaluate(Xs_test, ys_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "vggface_vgg16 (Model)        multiple                  14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "class_dense1 (Dense)         (None, 400)               10035600  \n",
            "_________________________________________________________________\n",
            "class_bn1 (BatchNormalizatio (None, 400)               1600      \n",
            "_________________________________________________________________\n",
            "class_act1 (Activation)      (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "class_drop1 (Dropout)        (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "class_dense2 (Dense)         (None, 100)               40100     \n",
            "_________________________________________________________________\n",
            "class_bn2 (BatchNormalizatio (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "class_act2 (Activation)      (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "class_drop2 (Dropout)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "class_dense_last (Dense)     (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "class_bn_last (BatchNormaliz (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "class_act_last (Activation)  (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 24,792,598\n",
            "Trainable params: 24,791,594\n",
            "Non-trainable params: 1,004\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2881547fdbbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mscore_Target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Xs_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_dfbPgstK6i",
        "colab_type": "text"
      },
      "source": [
        "# Only Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D16kvdCKtRhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def build_embedding_vgg(param, inp):\n",
        "    network = eval('VGGFace')\n",
        "    base = network(weights = 'vggface', include_top = False)\n",
        "    feat = base(inp)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "def build_classifier_vgg(param, embedding):\n",
        "    embedding = Input( (None, 25088) )\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "\n",
        "    densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return Model(embedding,actl)\n",
        "\n",
        "\n",
        "def train_classifier(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding_vgg(param, inp)\n",
        "    # for layer in embedding.layers[:]:\n",
        "    #     layer.trainable = False\n",
        "\n",
        "    classifier = build_classifier_vgg(param, embedding)\n",
        "    models[\"combined_classifier\"] = Model(inp,classifier(embedding))\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    # embedding.summary()\n",
        "    classifier.summary()\n",
        "    models[\"combined_classifier\"].summary()\n",
        "\n",
        "\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.4, random_state=42)\n",
        "    Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.4, random_state=42)\n",
        "\n",
        "\n",
        "    S_batches = batch_generator([Xs_train, ys_train], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt_train, np.zeros(shape = (len(Xt_train),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 60\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "\n",
        "    for i in range(1000):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        min_target_acc = 0\n",
        "        stats1 = models[\"combined_classifier\"].train_on_batch(Xsb, [ysb])\n",
        "        if ((i + 1) % 100 == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs_train)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt_train)\n",
        "\n",
        "\n",
        "            source_accuracy = accuracy_score(ys_train.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt_train.argmax(1), yt_pred.argmax(1))\n",
        "\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100)\n",
        "            print(log_str)\n",
        "            if(min_target_acc<target_accuracy):\n",
        "              min_target_acc = target_accuracy\n",
        "              classifier.save('drive/My Drive/final_proj_dataset/result2/model.h5')\n",
        "            \n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_train.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_train.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of iteration\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6M1ewVXb1w5",
        "colab_type": "code",
        "outputId": "80897247-952d-430f-b2cf-61a2fac6fcbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_classifier(param)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        (None, None, 25088)       0         \n",
            "_________________________________________________________________\n",
            "class_dense1 (Dense)         (None, None, 400)         10035600  \n",
            "_________________________________________________________________\n",
            "class_bn1 (BatchNormalizatio (None, None, 400)         1600      \n",
            "_________________________________________________________________\n",
            "class_act1 (Activation)      (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "class_drop1 (Dropout)        (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "class_dense2 (Dense)         (None, None, 100)         40100     \n",
            "_________________________________________________________________\n",
            "class_bn2 (BatchNormalizatio (None, None, 100)         400       \n",
            "_________________________________________________________________\n",
            "class_act2 (Activation)      (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "class_drop2 (Dropout)        (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "class_dense_last (Dense)     (None, None, 2)           202       \n",
            "_________________________________________________________________\n",
            "class_bn_last (BatchNormaliz (None, None, 2)           8         \n",
            "_________________________________________________________________\n",
            "class_act_last (Activation)  (None, None, 2)           0         \n",
            "=================================================================\n",
            "Total params: 10,077,910\n",
            "Trainable params: 10,076,906\n",
            "Non-trainable params: 1,004\n",
            "_________________________________________________________________\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_32 (InputLayer)        (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "vggface_vgg16 (Model)        multiple                  14714688  \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "model_21 (Model)             multiple                  10077910  \n",
            "=================================================================\n",
            "Total params: 24,792,598\n",
            "Trainable params: 24,791,594\n",
            "Non-trainable params: 1,004\n",
            "_________________________________________________________________\n",
            "iter: 00099: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 53.94737\n",
            "iter: 00199: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 44.07895\n",
            "iter: 00299: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 44.73684\n",
            "iter: 00399: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00499: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00599: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00699: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00799: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00899: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 48.02632\n",
            "iter: 00999: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 49.34211\n",
            "Source matrix:  [[73  0]\n",
            " [ 0 79]]\n",
            "Target matrix:  [[73  0]\n",
            " [77  2]]\n",
            "source accuracy test 0.8627450980392157\n",
            "target accuracy test 0.5294117647058824\n",
            "Source matrix:  [[47  7]\n",
            " [ 7 41]]\n",
            "Target matrix:  [[53  1]\n",
            " [47  1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQH37cTxOFF",
        "colab_type": "text"
      },
      "source": [
        "# Discriminative Method "
      ]
    }
  ]
}