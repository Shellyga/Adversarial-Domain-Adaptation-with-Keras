{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gan Working last chance Adversarial-Domain-Adaptation-with-Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shellyga/Adversarial-Domain-Adaptation-with-Keras/blob/master/Gan_Working_last_chance_Adversarial_Domain_Adaptation_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqYXgaotqk_g",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57CQpjm7OklS",
        "colab_type": "code",
        "outputId": "c9f94b59-7dcf-43cb-87cb-f8efb75ab20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "pip install keras_vggface"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_vggface\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/7d/5f0319ebdc09ac1a2272364fa9583f5067b6f8aff93fbbf8835d81cbaad7/keras_vggface-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.0.8)\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxcy4fLWqpgf",
        "colab_type": "code",
        "outputId": "d8f71d47-c67d-483c-a5ca-38fcc4fe1e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense,Reshape\n",
        "from keras.layers import BatchNormalization, Activation, Dropout\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def build_embedding(param, inp):\n",
        "    network = eval(param[\"network_name\"])\n",
        "    # base = network(weights = 'vggface', include_top = False)\n",
        "    base = network(weights = 'imagenet', include_top = False)\n",
        "    feat = base(inp)\n",
        "    print(feat.shape)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "# def build_embedding(param):\n",
        "#     model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), weights='vggface', pooling='avg')\n",
        "#     feat = model.get_layer('avg_pool').output\n",
        "#     # return feat\n",
        "#     # print(feat.shape)\n",
        "#     flat = Flatten()(feat)\n",
        "#     # flat = Reshape((-1,))(feat)\n",
        "#     # flat = Flatten()(embedding)\n",
        "#     # print(flat.shape)\n",
        "    # return flat\n",
        "\n",
        "def build_classifier(param, embedding):\n",
        "    # embedding = Input(embedding.shape)\n",
        "    embedding = Input( (None, 100352) )\n",
        "    # embedding = Reshape((-1,))(embedding)\n",
        "    # flat = Flatten()(embedding)\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "    # densel = Dense(param[\"number_of_classe\"], name = 'class_dense_last')(drop2)\n",
        "    densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return Model(input=embedding, outputs=actl)\n",
        "\n",
        "def build_discriminator(param, embedding):\n",
        "    # embedding = Input(embedding.shape)\n",
        "    embedding = Input( (None, 100352) )\n",
        "    # embedding = Reshape((-1,))(embedding)\n",
        "    # flat = Flatten()(embedding)\n",
        "    dense1 = Dense(400, name = 'dis_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name='dis_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'dis_act1')(bn1)\n",
        "    drop1 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'dis_dense2')(drop1)\n",
        "    bn2 = BatchNormalization(name='dis_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'dis_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop2')(act2)\n",
        "\n",
        "    densel = Dense(1, name = 'dis_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'dis_bn_last')(densel)\n",
        "    actl = Activation('sigmoid', name = 'dis_act_last')(bnl)\n",
        "    return Model(input=embedding, outputs=actl)\n",
        "\n",
        "def build_combined_classifier(inp, classifier):\n",
        "    comb_model = Model(inputs = inp, outputs = [classifier])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_discriminator(inp, discriminator):\n",
        "    comb_model = Model(inputs = inp, outputs = [discriminator])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_model(inp, comb):\n",
        "    comb_model = Model(inputs = inp, outputs = comb)\n",
        "    return comb_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DrTIu1SqtIe",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8K7kUmqu5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def opt_classifier(param):\n",
        "    return Adam(lr=param[\"lr_classifier\"], beta_1=param[\"b1_classifier\"], beta_2=param[\"b2_classifier\"])\n",
        "\n",
        "def opt_discriminator(param):\n",
        "    return Adam(lr=param[\"lr_discriminator\"], beta_1=param[\"b1_discriminator\"], beta_2=param[\"b2_discriminator\"])\n",
        "\n",
        "def opt_combined(param):\n",
        "    return Adam(lr=param[\"lr_combined\"], beta_1=param[\"b1_combined\"], beta_2=param[\"b2_combined\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHRxvLjYqwvG",
        "colab_type": "text"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKrPVLPi-b9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.regularizers import l2\n",
        "# from imutils.paths import list_images\n",
        "# from imutils import paths\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# SEED = 7\n",
        "# import os\n",
        "# import sys\n",
        "# import argparse\n",
        "# import random\n",
        "# import numpy as np\n",
        "# # import tensorflow.python.keras as tf\n",
        "# from tensorflow.compat.v1 import set_random_seed\n",
        "# # import tensorflow.python.keras as tf\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn import metrics\n",
        "# from plotnine import *\n",
        "# import pandas as pd\n",
        "                \n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "# from math import ceil\n",
        "# from sklearn.utils import shuffle as skshuffle\n",
        "\n",
        "# os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# set_random_seed(SEED)\n",
        "# random.seed(SEED)\n",
        "\n",
        "# from PIL import Image\n",
        "# from keras.utils import to_categorical\n",
        "# from keras.layers import Input\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils import multi_gpu_model\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# def load_data(dataset_s,dataset_t):\n",
        "#     # initialize the data and labels\n",
        "#     depth, height, width = 3, 224, 224\n",
        "\n",
        "#     reg = l2(0.0005)\n",
        "#     print(\"[INFO] loading images...\")\n",
        "#     data_t = []\n",
        "#     data_s = []\n",
        "#     labels_s = []\n",
        "#     labels_t = []\n",
        "#     print(\"****************** im train_nn\")\n",
        "\n",
        "#     # grab the image paths\n",
        "#     imagePaths_s = sorted(list(paths.list_images(dataset_s)))\n",
        "#     imagePaths_t = sorted(list(paths.list_images(dataset_t)))\n",
        "\n",
        "\n",
        "#     # loop over the input images\n",
        "#     for imagePath in imagePaths_s:\n",
        "#         # extract the class label from the image path and update the\n",
        "#         # labels list\n",
        "#         label = imagePath.split(os.path.sep)[-2]\n",
        "#         if label == \"model\":\n",
        "#             continue\n",
        "#         labels_s.append(label)\n",
        "    \n",
        "#         # load the image, resize the image to be height X width pixels (ignoring\n",
        "#         # aspect ratio), flatten the image into (height * width * 3) pixel image\n",
        "#         # into a list, and store the image in the data list\n",
        "#         image = cv2.imread(imagePath)\n",
        "#         image = cv2.resize(image, (height, width)).flatten()\n",
        "#         data_s.append(image)\n",
        "\n",
        "#     for imagePath in imagePaths_t:\n",
        "#         # extract the class label from the image path and update the\n",
        "#         label = imagePath.split(os.path.sep)[-2]\n",
        "#         if label == \"model\":\n",
        "#             continue\n",
        "#         labels_t.append(label)\n",
        "\n",
        "#         # load the image, resize the image to be height X width pixels (ignoring\n",
        "#         # aspect ratio), flatten the image into (height * width * 3) pixel image\n",
        "#         # into a list, and store the image in the data list\n",
        "#         image = cv2.imread(imagePath)\n",
        "#         image = cv2.resize(image, (height, width)).flatten()\n",
        "#         data_t.append(image)\n",
        "\n",
        "#     data_s, labels_s = skshuffle(data_s, labels_s)\n",
        "\n",
        "#     data_t, labels_t = skshuffle(data_t, labels_t)\n",
        "\n",
        "#     # scale the raw pixel intensities to the range [0, 1]\n",
        "#     data_s = np.array(data_s, dtype=\"float\") / 255.0\n",
        "#     labels_s = np.array(labels_s)\n",
        "\n",
        "#     data_t = np.array(data_t, dtype=\"float\") / 255.0\n",
        "#     labels_t = np.array(labels_t)\n",
        "\n",
        "#     return data_s,labels_s,data_t,labels_t\n",
        "  \n",
        "\n",
        "# def batch_generator(data, batch_size):\n",
        "#     #Generate batches of data.\n",
        "#     all_examples_indices = len(data[0])\n",
        "#     while True:\n",
        "#         mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "#         tbr = [k[mini_batch_indices] for k in data]\n",
        "#         yield tbr\n",
        "\n",
        "# def one_hot_encoding(param):\n",
        "#     lb = LabelEncoder()\n",
        "#     source_labels,target_labels  = lb.fit_transform( param[\"source_label\"]),lb.fit_transform( param[\"target_label\"])\n",
        "#     param[\"number_of_classe\"] = len(lb.classes_)\n",
        "#     return source_labels,target_labels \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlgU5w6qyJO",
        "colab_type": "code",
        "outputId": "536e80af-24d2-493a-d842-88f3fdc9ddaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvsXa1TXNh5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 7\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "# import tensorflow.python.keras as tf\n",
        "from tensorflow.compat.v1 import set_random_seed\n",
        "# import tensorflow.python.keras as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from plotnine import *\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "from sklearn.utils import shuffle as skshuffle\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # print(path)\n",
        "    # Return the RGB variant of input image\n",
        "    with open(path, 'rb') as f:\n",
        "      with Image.open(f) as img:\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def one_hot_encoding(param):\n",
        "    # Read the source and target labels from param\n",
        "    s_label = param[\"source_label\"]\n",
        "    t_label = param[\"target_label\"]\n",
        "\n",
        "    # Encode the labels into one-hot format\n",
        "    classes = (np.concatenate((s_label, t_label), axis = 0))\n",
        "    num_classes = np.max(classes)\n",
        "    if 0 in classes:\n",
        "            num_classes = num_classes+1\n",
        "    s_label = to_categorical(s_label, num_classes = num_classes)\n",
        "    t_label = to_categorical(t_label, num_classes = num_classes)\n",
        "    return s_label, t_label\n",
        "            \n",
        "def data_loader(filepath, inp_dims):\n",
        "    # Load images and corresponding labels from the text file, stack them in numpy arrays and return\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
        "        # sys.exit() \n",
        "    img = []\n",
        "    label = []\n",
        "    with open(filepath,'r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "            token = line.split()\n",
        "            image_path = \"drive/My Drive/final_proj_dataset/\"+token[0]\n",
        "            i = pil_loader(image_path)\n",
        "            i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "            detector = dlib.get_frontal_face_detector()\n",
        "            predictor = \"drive/My Drive/final_proj_dataset/shape_predictor_5_face_landmarks.dat\"\n",
        "            frame_1 = align_and_crop(detector, np.array(i), predictor)[0]\n",
        "            frame_1 = cv2.resize(frame_1,(inp_dims[0], inp_dims[1]))\n",
        "            img.append(frame_1)\n",
        "            label.append(int(token[1]))\n",
        "    # img, label = skshuffle(img, label)\n",
        "    img = np.array(img)\n",
        "    label = np.array(label)\n",
        "    return img, label\n",
        "\n",
        "# def data_loader(filepath, inp_dims):\n",
        "#     img = []\n",
        "#     labels = []\n",
        "#     # grab the image paths\n",
        "#     imagePaths = sorted(list(paths.list_images(filepath)))\n",
        "#     # loop over the input images\n",
        "#     for imagePath in imagePaths:\n",
        "#         # extract the class label from the image path and update the\n",
        "#         # labels list\n",
        "#         label = imagePath.split(os.path.sep)[-2]\n",
        "#         labels.append(label)\n",
        "\n",
        "#         i = pil_loader(imagePath)\n",
        "#         i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "#         detector = dlib.get_frontal_face_detector()\n",
        "#         predictor = \"drive/My Drive/final_proj_dataset/shape_predictor_5_face_landmarks.dat\"\n",
        "#         frame_1 = align_and_crop(detector, np.array(i), predictor)[0]\n",
        "#         frame_1 = cv2.resize(frame_1,(inp_dims[0], inp_dims[1]))\n",
        "#         img.append(frame_1)\n",
        "\n",
        "#     img, labels = skshuffle(img, labels)\n",
        "#     img = np.array(img)\n",
        "#     labels = np.array(labels)\n",
        "#     return img, labels\n",
        "    \n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    #Generate batches of data.\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr\n",
        "\n",
        "\n",
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp)\n",
        "    # embedding = build_embedding(param)\n",
        "    # inp_embedding = Input(shape = embedding.shape))\n",
        "    classifier = build_classifier(param, embedding)\n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "    \n",
        "    models['combined_classifier'] = Model(inputs=inp, outputs=classifier(embedding))\n",
        "    # models[\"combined_discriminator\"] = build_combined_discriminator(inp, discriminator)\n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    models[\"combined_discriminator\"].trainable = False\n",
        "    \n",
        "    models[\"combined_classifier\"] = build_combined_classifier(inp, classifier)\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    models[\"combined_model\"] = build_combined_model(inp, [classifier, discriminator]) \n",
        "    models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = {'class_act_last': 'categorical_crossentropy', 'dis_act_last': \\\n",
        "        'binary_crossentropy'}, loss_weights = {'class_act_last': param[\"class_loss_weight\"], 'dis_act_last': param[\"dis_loss_weight\"]}, metrics = ['accuracy'])\n",
        "\n",
        "    models[\"combined_classifier\"].summary()\n",
        "    models[\"combined_discriminator\"].summary()\n",
        "    models[\"combined_model\"].summary()\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt, Xt_test, yt, yt_test = train_test_split(Xt, yt, test_size=0.2, random_state=42)\n",
        "    Xs, Xs_test, ys, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Xs_test, ys_test = param[\"source_data_test\"], param[\"source_label_test\"]\n",
        "    # Xt_test, yt_test = param[\"target_data_test\"], param[\"target_label_test\"]\n",
        "    Xs, ys = aug_training_set_loader(Xs, ys, param[\"inp_dims\"])\n",
        "    Xt, yt = aug_training_set_loader(Xt, yt, param[\"inp_dims\"])\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "    loss_discriminator = []\n",
        "    loss_classifier_source=[]\n",
        "\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "\n",
        "\n",
        "        # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        d_loss = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "        # d_loss_source = models[\"combined_discriminator\"].train_on_batch(Xsb, [ysb])\n",
        "        # d_loss_target = models[\"combined_discriminator\"].train_on_batch(Xtb, ytb)\n",
        "        # d_loss = 0.5 * np.add(d_loss_source, d_loss_target)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1])\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt)\n",
        "            \n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "            acc_domain_source.append(source_domain_accuracy)\n",
        "            acc_domain_target.append(target_domain_accuracy)\n",
        "\n",
        "            # loss_discriminator.append(stats2)\n",
        "            # loss_classifier_source.append(stats1)\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                optim[\"labels\"] = ys_pred.argmax(1)\n",
        "                param[\"target_accuracy\"] < target_accuracy\n",
        "\n",
        "                if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "                    gap_last_snap = 0\n",
        "                    np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "                    open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "                #     models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        gap_last_snap = gap_last_snap + 1;\n",
        "    \n",
        "    print(optim[\"iter\"],optim[\"acc\"],optim[\"labels\"])\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "    # ys_test_adv_pred = models[\"combined_discriminator\"].predict(Xs_test)\n",
        "    # yt_test_adv_pred = models[\"combined_discriminator\"].predict(Xt_test)\n",
        "    # domain_source_accuracy_test = accuracy_score(ys_adv, np.round(ys_test_adv_pred))              \n",
        "    # domain_target_accuracy_test = accuracy_score(yt_adv, np.round(yt_test_adv_pred))\n",
        "\n",
        "    # print(\"discriminator source accuracy test\",domain_source_accuracy_test)\n",
        "    # print(\"discriminator target accuracy test\",domain_target_accuracy_test)\n",
        "\n",
        "    # print(param[\"num_iterations\"]/ (param[\"test_interval\"]-1))\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_source), label=\"Domain Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_target), label=\"Domain Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # plt.style.use(\"ggplot\")\n",
        "    # plt.figure()\n",
        "    # plt.plot(N, np.array(loss_classifier_source), label=\"Classifier Loss\")\n",
        "    # plt.plot(N, np.array(loss_discriminator), label=\"Discriminator loss\")\n",
        "    # plt.title(\"Training Loss of Source and Target \")\n",
        "    # plt.xlabel(\"Number of intervals\")\n",
        "    # plt.ylabel(\"Loss\")\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZPFQrHKAotL",
        "colab_type": "text"
      },
      "source": [
        "# Changes I made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwuSxj4hW9vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 7\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "# import tensorflow.python.keras as tf\n",
        "from tensorflow.compat.v1 import set_random_seed\n",
        "# import tensorflow.python.keras as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from plotnine import *\n",
        "import pandas as pd\n",
        "                \n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "from sklearn.utils import shuffle as skshuffle\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # print(path)\n",
        "    # Return the RGB variant of input image\n",
        "    with open(path, 'rb') as f:\n",
        "      with Image.open(f) as img:\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def one_hot_encoding(param):\n",
        "    # Read the source and target labels from param\n",
        "    s_label = param[\"source_label\"]\n",
        "    t_label = param[\"target_label\"]\n",
        "\n",
        "    # Encode the labels into one-hot format\n",
        "    classes = (np.concatenate((s_label, t_label), axis = 0))\n",
        "    num_classes = np.max(classes)\n",
        "    if 0 in classes:\n",
        "            num_classes = num_classes+1\n",
        "    s_label = to_categorical(s_label, num_classes = num_classes)                \n",
        "    t_label = to_categorical(t_label, num_classes = num_classes)\n",
        "    return s_label, t_label\n",
        "            \n",
        "def data_loader(filepath, inp_dims):\n",
        "    # Load images and corresponding labels from the text file, stack them in numpy arrays and return\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
        "        # sys.exit() \n",
        "    img = []\n",
        "    label = []\n",
        "    with open(filepath,'r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "            token = line.split()\n",
        "            image_path = \"drive/My Drive/final_proj_dataset/\"+token[0]\n",
        "            i = pil_loader(image_path)\n",
        "            i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "            detector = dlib.get_frontal_face_detector()\n",
        "            predictor = \"drive/My Drive/final_proj_dataset/shape_predictor_5_face_landmarks.dat\"\n",
        "            frame_1 = align_and_crop(detector, np.array(i), predictor)[0]\n",
        "            frame_1 = cv2.resize(frame_1,(inp_dims[0], inp_dims[1]))\n",
        "            img.append(frame_1)\n",
        "            label.append(int(token[1]))\n",
        "    img, label = skshuffle(img, label)\n",
        "    img = np.array(img)\n",
        "    label = np.array(label)\n",
        "    return img, label\n",
        "    \n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    #Generate batches of data.\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE5GvMe6AsOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp) \n",
        "    # embedding = build_embedding(param)   \n",
        "    \n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "\n",
        "    # The discriminator takes the representaton as input and determines the domain\n",
        "    models[\"combined_discriminator\"] = Model(inputs=inp, outputs=discriminator(embedding))\n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    # For the combined model we will only train the generator\n",
        "    discriminator.trainable = False\n",
        "  \n",
        "    # # Build the classifier\n",
        "    classifier = build_classifier(param, embedding)\n",
        "    models['combined_classifier'] = Model(inputs=inp, outputs=classifier(embedding))\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # #freezed clasifer \n",
        "    # # load and evaluate a saved model\n",
        "    # from numpy import loadtxt\n",
        "    # from keras.models import load_model\n",
        "\n",
        "    # model_path = 'drive/My Drive/final_proj_dataset/result2/model.h5'\n",
        "    # # load model\n",
        "    # models[\"combined_classifier\"] = load_model(model_path)\n",
        "    # # summarize model.\n",
        "    # models[\"combined_classifier\"].summary()\n",
        "    # # load dataset\n",
        "\n",
        "    # score_Target = models[\"combined_classifier\"].evaluate(param[\"target_data\"], param[\"target_label\"], verbose=0)\n",
        "    # print(\"%s: %.2f%%\" % (models[\"combined_classifier\"].metrics_names[1], score_Target[1]*100))\n",
        "\n",
        "    \n",
        "    models[\"combined_model\"] = build_combined_model(inp, [classifier(embedding), discriminator(embedding)]) \n",
        "    models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = ['categorical_crossentropy', 'binary_crossentropy'] , loss_weights =  [param[\"class_loss_weight\"],  param[\"dis_loss_weight\"]], metrics = ['accuracy'])\n",
        "\n",
        "    models[\"combined_classifier\"].summary()\n",
        "    models[\"combined_discriminator\"].summary()\n",
        "    models[\"combined_model\"].summary()\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt, Xt_test, yt, yt_test = train_test_split(Xt, yt, test_size=0.2, random_state=42)\n",
        "    Xs, Xs_test, ys, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Xs_test, ys_test = param[\"source_data_test\"], param[\"source_label_test\"]\n",
        "    # Xt_test, yt_test = param[\"target_data_test\"], param[\"target_label_test\"]\n",
        "    Xs, ys = aug_training_set_loader(Xs, ys, param[\"inp_dims\"])\n",
        "    Xt, yt = aug_training_set_loader(Xt, yt, param[\"inp_dims\"])\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "    loss_discriminator = []\n",
        "    loss_classifier_source=[]\n",
        "\n",
        "    t_domain_label = np.ones(( param[\"batch_size\"], 1))\n",
        "    s_domain_label = np.zeros(( param[\"batch_size\"], 1))\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "        # Extract a batch of new features????????????\n",
        "        # s_features = models[\"combined_discriminator\"].predict(Xsb)\n",
        "        # t_features = models[\"combined_discriminator\"].predict(Xtb)\n",
        "\n",
        "        # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        # d_loss_s = models[\"combined_discriminator\"].train_on_batch(Xsb, s_domain_label)\n",
        "        # d_loss_t = models[\"combined_discriminator\"].train_on_batch(Xtb, t_domain_label)\n",
        "        # d_loss = 0.5 * np.add(d_loss_s, d_loss_t)\n",
        "        if ((i+1 ) % 2 == 0):\n",
        "          d_loss = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss_s = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1])\n",
        "\n",
        "        # # Train the discriminator (real classified as ones and generated as zeros)\n",
        "        # d_loss = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "        # # d_loss_source = models[\"combined_discriminator\"].train_on_batch(Xsb, [ysb])\n",
        "        # # d_loss_target = models[\"combined_discriminator\"].train_on_batch(Xtb, ytb)\n",
        "        # # d_loss = 0.5 * np.add(d_loss_source, d_loss_target)\n",
        "\n",
        "        # # ---------------------\n",
        "        # #  Train Generator\n",
        "        # # ---------------------\n",
        "\n",
        "        # # Train the generator (wants discriminator to mistake images as real)\n",
        "        # g_loss = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1])\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt)\n",
        "            \n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "            acc_domain_source.append(source_domain_accuracy)\n",
        "            acc_domain_target.append(target_domain_accuracy)\n",
        "\n",
        "            # loss_discriminator.append(stats2)\n",
        "            # loss_classifier_source.append(stats1)\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                optim[\"labels\"] = ys_pred.argmax(1)\n",
        "                param[\"target_accuracy\"] = target_accuracy\n",
        "\n",
        "        #         if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "        #             gap_last_snap = 0\n",
        "        #             np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "        #             open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "        #         #     models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        # gap_last_snap = gap_last_snap + 1;\n",
        "    \n",
        "    print(optim[\"iter\"],optim[\"acc\"],optim[\"labels\"])\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "    # ys_test_adv_pred = models[\"combined_discriminator\"].predict(Xs_test)\n",
        "    # yt_test_adv_pred = models[\"combined_discriminator\"].predict(Xt_test)\n",
        "    # domain_source_accuracy_test = accuracy_score(ys_adv, np.round(ys_test_adv_pred))              \n",
        "    # domain_target_accuracy_test = accuracy_score(yt_adv, np.round(yt_test_adv_pred))\n",
        "\n",
        "    # print(\"discriminator source accuracy test\",domain_source_accuracy_test)\n",
        "    # print(\"discriminator target accuracy test\",domain_target_accuracy_test)\n",
        "\n",
        "    # print(param[\"num_iterations\"]/ (param[\"test_interval\"]-1))\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_source), label=\"Domain Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_domain_target), label=\"Domain Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of intervals\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # plt.style.use(\"ggplot\")\n",
        "    # plt.figure()\n",
        "    # plt.plot(N, np.array(loss_classifier_source), label=\"Classifier Loss\")\n",
        "    # plt.plot(N, np.array(loss_discriminator), label=\"Discriminator loss\")\n",
        "    # plt.title(\"Training Loss of Source and Target \")\n",
        "    # plt.xlabel(\"Number of intervals\")\n",
        "    # plt.ylabel(\"Loss\")\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdnIND2JEHUn",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq_tQgLEEETm",
        "colab_type": "code",
        "outputId": "da32a06e-1370-459f-8fb2-b99a0a283331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import argparse\n",
        "if __name__ == \"__main__\":\n",
        "    # Read parameter values from the console\n",
        "    parser = argparse.ArgumentParser(description = 'Domain Adaptation')\n",
        "    parser.add_argument('--number_of_gpus', type = int, nargs = '?', default = '1', help = \"Number of gpus to run\")\n",
        "    parser.add_argument('--network_name', type = str, default = 'ResNet50', help = \"Name of the feature extractor network\")\n",
        "    parser.add_argument('--dataset_name', type = str, default = 'Office', help = \"Name of the source dataset\")\n",
        "    parser.add_argument('--dropout_classifier', type = float, default = 0.25, help = \"Dropout ratio for classifier\")\n",
        "    parser.add_argument('--dropout_discriminator', type = float, default = 0.25, help = \"Dropout ratio for discriminator\")    \n",
        "    parser.add_argument('--source_path', type = str, default = 'amazon_10_list.txt', help = \"Path to source dataset\")\n",
        "    parser.add_argument('--target_path', type = str, default = 'webcam_10_list.txt', help = \"Path to target dataset\")\n",
        "    parser.add_argument('--lr_classifier', type = float, default = 0.0001, help = \"Learning rate for classifier model\")\n",
        "    parser.add_argument('--b1_classifier', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for classifier model optimizer\")\n",
        "    parser.add_argument('--b2_classifier', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for classifier model optimizer\")\n",
        "    parser.add_argument('--lr_discriminator', type = float, default = 0.00001, help = \"Learning rate for discriminator model\")\n",
        "    parser.add_argument('--b1_discriminator', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for discriminator model optimizer\")\n",
        "    parser.add_argument('--b2_discriminator', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for discriminator model optimizer\")\n",
        "    parser.add_argument('--lr_combined', type = float, default = 0.00001, help = \"Learning rate for combined model\")\n",
        "    parser.add_argument('--b1_combined', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for combined model optimizer\")\n",
        "    parser.add_argument('--b2_combined', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for combined model optimizer\")\n",
        "    parser.add_argument('--classifier_loss_weight', type = float, default = 1, help = \"Classifier loss weight\")\n",
        "    parser.add_argument('--discriminator_loss_weight', type = float, default = 4, help = \"Discriminator loss weight\")\n",
        "    parser.add_argument('--batch_size', type = int, default = 32, help = \"Batch size for training\")\n",
        "    parser.add_argument('--test_interval', type = int, default = 3, help = \"Gap between two successive test phases\")\n",
        "    parser.add_argument('--num_iterations', type = int, default = 120, help = \"Number of iterations\")\n",
        "    parser.add_argument('--snapshot_interval', type = int, default = 500, help = \"Minimum gap between saving outputs\")\n",
        "    parser.add_argument('--output_dir', type = str, default = 'Models', help = \"Directory for saving outputs\")\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Set GPU device\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(list(np.arange(args.number_of_gpus))).strip('[]')\n",
        "\n",
        "    # Initialize parameters\n",
        "    param = {}\n",
        "    param[\"number_of_gpus\"] = 1\n",
        "    param[\"network_name\"] = 'ResNet50'\n",
        "    # param[\"network_name\"] = 'VGGFace'\n",
        "    param[\"inp_dims\"] = [224, 224, 3]\n",
        "    param[\"num_iterations\"] = 1000\n",
        "    # param[\"num_iterations\"] = 500\n",
        "    param[\"lr_classifier\"] = 0.0001\n",
        "    param[\"b1_classifier\"] = 0.9\n",
        "    param[\"b2_classifier\"] = 0.999    \n",
        "    param[\"lr_discriminator\"] = 0.00001\n",
        "    param[\"b1_discriminator\"] =  0.9\n",
        "    param[\"b2_discriminator\"] = 0.999\n",
        "    param[\"lr_combined\"] = 0.00001\n",
        "    param[\"b1_combined\"] =  0.9\n",
        "    param[\"b2_combined\"] =  0.999       \n",
        "    # param[\"batch_size\"] = int(32)\n",
        "    param[\"batch_size\"] = int(32/2)\n",
        "    param[\"class_loss_weight\"] = 1\n",
        "    param[\"dis_loss_weight\"] = 4    \n",
        "    param[\"drop_classifier\"] = 0.25\n",
        "    param[\"drop_discriminator\"] = 0.25\n",
        "    param[\"test_interval\"] = 100\n",
        "    param[\"source_path\"] = 'drive/My Drive/final_proj_dataset/data_file_shelly.txt'\n",
        "    param[\"target_path\"] = 'drive/My Drive/final_proj_dataset/data_file_yerus.txt' \n",
        "    # param[\"source_path\"] = 'drive/My Drive/pro_data/dataset_shelly'\n",
        "    # param[\"target_path\"] = 'drive/My Drive/pro_data/dataset_yerus' \n",
        "    param[\"snapshot_interval\"] = 500\n",
        "    # param[\"snapshot_interval\"] = 5\n",
        "    param[\"output_path\"] = 'drive/My Drive/final_proj_dataset/result2'\n",
        "    param[\"number_of_classe\"] = 0\n",
        "\n",
        "    # # Create directory for saving models and log files\n",
        "    if not os.path.exists(param[\"output_path\"]):\n",
        "        os.mkdir(param[\"output_path\"])\n",
        "    \n",
        "    print(\"[INFO] loading images...\")\n",
        "    # Load source and target data\n",
        "    param[\"source_data\"], param[\"source_label\"] = data_loader(param[\"source_path\"], param[\"inp_dims\"])\n",
        "    param[\"target_data\"], param[\"target_label\"] = data_loader(param[\"target_path\"], param[\"inp_dims\"])\n",
        "    # param[\"source_data\"], param[\"source_label\"],param[\"target_data\"], param[\"target_label\"] =  load_data(param[\"source_path\"],param[\"target_path\"])\n",
        "\n",
        "    # Encode labels into one-hot format\n",
        "    param[\"source_label\"], param[\"target_label\"] = one_hot_encoding(param)\n",
        "\n",
        "    \n",
        "\n",
        "   "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vaKAXuoj3A",
        "colab_type": "code",
        "outputId": "c4cedf27-c0fc-46ff-dbd1-9d984c31f95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train data\n",
        "print(\"[INFO] training network...\")\n",
        "train(param)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 8s 0us/step\n",
            "(None, 7, 7, 2048)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"di..., inputs=Tensor(\"in...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"cl..., inputs=Tensor(\"in...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             multiple                  23587712  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "model_3 (Model)              multiple                  40183510  \n",
            "=================================================================\n",
            "Total params: 63,771,222\n",
            "Trainable params: 63,717,098\n",
            "Non-trainable params: 54,124\n",
            "_________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             multiple                  23587712  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "model_1 (Model)              multiple                  40183405  \n",
            "=================================================================\n",
            "Total params: 103,953,520\n",
            "Trainable params: 63,716,995\n",
            "Non-trainable params: 40,236,525\n",
            "_________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "resnet50 (Model)                multiple             23587712    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100352)       0           resnet50[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_3 (Model)                 multiple             40183510    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 multiple             40183405    flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 103,954,627\n",
            "Trainable params: 63,717,098\n",
            "Non-trainable params: 40,237,529\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter: 00099: \n",
            "LABEL CLASSIFICATION: source_accuracy: 92.69294, target_accuracy: 47.53695                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 97.20854, target_domain_accuracy: 100.00000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCAxthiKjwnr",
        "colab_type": "text"
      },
      "source": [
        "# Augment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD18oqysj0hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "from imgaug import seed\n",
        "def aug_training_set_loader(images,labels,inp_dims):\n",
        "    NUM_COPIES = ceil(1000/len(images))\n",
        "    images = np.array(images)\n",
        "    images=augment(inp_dims[0], inp_dims[1], images, NUM_COPIES)\n",
        "    aug_labels =[]\n",
        "    for x in labels:\n",
        "        for i in range(NUM_COPIES+1):  # NUM_COPIES+1-> numbers of augmentations + 1 original\n",
        "            aug_labels.append(x)\n",
        "    images, labels = skshuffle(images, aug_labels)\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "\n",
        "def augment(width, height, data, NUM_COPIES):\n",
        "    \"\"\"\n",
        "    preform augmentetion on the list of photos named 'data'\n",
        "    :param width: width of a single photo\n",
        "    :param height: height of a single photo\n",
        "    :param data: a list of photos to augment\n",
        "    :param NUM_COPIES: number of copies produced from the image\n",
        "    :return: a list of photos that consists from the original photos and their augmentations\n",
        "    \"\"\"\n",
        "    augmented_data = []\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "    seq = iaa.Sequential([\n",
        "        sometimes(iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))),\n",
        "        sometimes(iaa.Fliplr()),  # horizontal flips\n",
        "        sometimes(iaa.AddElementwise((-50, 50))),\n",
        "        sometimes(iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0.0, 2.0)))),\n",
        "        sometimes(iaa.ContrastNormalization((0.8, 1.2))),\n",
        "        sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n",
        "        sometimes(iaa.Affine(\n",
        "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "            shear=(-8, 8)\n",
        "        )),\n",
        "        sometimes(iaa.Superpixels(p_replace=0.1, n_segments=150))\n",
        "    ], random_order=True)  # apply augmenters in random order\n",
        "    for img in data:\n",
        "        copies = augment_image(width, height, img, seq, NUM_COPIES)\n",
        "        copies.append(img.reshape(width, height, 3))\n",
        "        for cpy in copies:\n",
        "            augmented_data.append(cpy)\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "def augment_image(width, height, image, seq, NUM_COPIES):\n",
        "    \"\"\"\n",
        "        augments a single image\n",
        "    :param width: width of the image\n",
        "    :param height: height of the image\n",
        "    :param image: a matrix representing a rgb image\n",
        "    :param seq: the augmantation sequence preformed\n",
        "    :param NUM_COPIES: number of copies produced from the image\n",
        "    :return: a list of all images made from 'image' not including the original\n",
        "    \"\"\"\n",
        "    seed (1)\n",
        "    copies = []\n",
        "    image = image.reshape(width, height, 3)\n",
        "    for i in range(NUM_COPIES):\n",
        "        copies.append(seq.augment_image(image))\n",
        "\n",
        "    return copies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqiFjCePj2a1",
        "colab_type": "text"
      },
      "source": [
        "# Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSCoB73kj6ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# def resource_path(filename):\n",
        "#     \"\"\" Get absolute path to resource, for the executable\"\"\"\n",
        "#     if getattr(sys, 'frozen', False):\n",
        "#         # The application is frozen\n",
        "#         datadir = os.path.dirname(sys.executable)\n",
        "#     else:\n",
        "#         # The application is not frozen\n",
        "#         # Change this bit to match where you store your data files:\n",
        "#         datadir = os.path.dirname(__file__)\n",
        "\n",
        "    # return os.path.join(datadir, filename)\n",
        "\n",
        "def align_and_crop(detector, im_to_align, predictor_path):\n",
        "    '''\n",
        "    simply aligns the photo (using the detector to identify the eyes) and crops the face.\n",
        "    :param im_to_align: an address of an image\n",
        "    '''\n",
        "\n",
        "    # a shape predictor to find face landmarks so we can precisely localize the face\n",
        "    sp = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
        "    # second argument indicates that we should upsample the image 1 time.\n",
        "    try:\n",
        "        dets = detector(im_to_align, 1)\n",
        "    except RuntimeError:\n",
        "        return -1\n",
        "\n",
        "    num_faces = len(dets)\n",
        "    if num_faces == 0:\n",
        "        return im_to_align, False\n",
        "\n",
        "    # Find the 5 face landmarks we need to do the alignment.\n",
        "    faces = dlib.full_object_detections()\n",
        "    for detection in dets:\n",
        "        faces.append(sp(im_to_align, detection))\n",
        "\n",
        "    # get a single chip (aligned and cropped)\n",
        "    image = dlib.get_face_chip(im_to_align, faces[0])\n",
        "    # cv2.imshow(\"f\", image) show image for testing\n",
        "    return image, True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA7HFj4Rzgj0",
        "colab_type": "text"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EITQL6jazlp8",
        "colab_type": "code",
        "outputId": "7afec2b3-f923-4208-c06a-5aabe0cf6016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "from keras.models import load_model\n",
        "\n",
        "model_path = 'drive/My Drive/final_proj_dataset/result2/model.h5'\n",
        "# load model\n",
        "model = load_model(model_path)\n",
        "# summarize model.\n",
        "model.summary()\n",
        "# load dataset\n",
        "\n",
        "score_Target = model.evaluate(Xs_test, ys_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "vggface_vgg16 (Model)        multiple                  14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "class_dense1 (Dense)         (None, 400)               10035600  \n",
            "_________________________________________________________________\n",
            "class_bn1 (BatchNormalizatio (None, 400)               1600      \n",
            "_________________________________________________________________\n",
            "class_act1 (Activation)      (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "class_drop1 (Dropout)        (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "class_dense2 (Dense)         (None, 100)               40100     \n",
            "_________________________________________________________________\n",
            "class_bn2 (BatchNormalizatio (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "class_act2 (Activation)      (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "class_drop2 (Dropout)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "class_dense_last (Dense)     (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "class_bn_last (BatchNormaliz (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "class_act_last (Activation)  (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 24,792,598\n",
            "Trainable params: 24,791,594\n",
            "Non-trainable params: 1,004\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2881547fdbbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mscore_Target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Xs_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_dfbPgstK6i",
        "colab_type": "text"
      },
      "source": [
        "# Only Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D16kvdCKtRhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def build_embedding_vgg(param, inp):\n",
        "    network = eval('VGGFace')\n",
        "    base = network(weights = 'vggface', include_top = False)\n",
        "    feat = base(inp)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "def build_classifier_vgg(param, embedding):\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "\n",
        "    densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return actl\n",
        "\n",
        "\n",
        "def train_classifier(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding_vgg(param,inp)\n",
        "    classifier = build_classifier_vgg(param, embedding)\n",
        "    \n",
        "    models[\"combined_classifier\"] = Model(inp,classifier)\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size=0.25, random_state=42)\n",
        "    Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "    S_batches = batch_generator([Xs_train, ys_train], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt_train, np.zeros(shape = (len(Xt_train),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 60\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    acc_source=[]\n",
        "    acc_target=[]\n",
        "    acc_domain_source=[]\n",
        "    acc_domain_target=[]\n",
        "\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        " \n",
        "\n",
        "        stats1 = models[\"combined_classifier\"].train_on_batch(Xsb, [ysb])\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs_train)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt_train)\n",
        "\n",
        "\n",
        "            source_accuracy = accuracy_score(ys_train.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt_train.argmax(1), yt_pred.argmax(1))\n",
        "\n",
        "\n",
        "            acc_source.append(source_accuracy)\n",
        "            acc_target.append(target_accuracy)\n",
        "\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            models[\"combined_classifier\"].save('drive/My Drive/final_proj_dataset/result2/model.h5')\n",
        "            \n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_train.argmax(1), ys_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_train.argmax(1), yt_pred.argmax(1)))\n",
        "\n",
        "    ys_test_pred = models[\"combined_classifier\"].predict(Xs_test)\n",
        "    yt_test_pred = models[\"combined_classifier\"].predict(Xt_test)\n",
        "    source_accuracy_test = accuracy_score(ys_test.argmax(1), ys_test_pred.argmax(1))              \n",
        "    target_accuracy_test = accuracy_score(yt_test.argmax(1), yt_test_pred.argmax(1))\n",
        "\n",
        "    print(\"source accuracy test\",source_accuracy_test)\n",
        "    print(\"target accuracy test\",target_accuracy_test)\n",
        "\n",
        "    print(\"Source matrix: \",metrics.confusion_matrix(ys_test.argmax(1), ys_test_pred.argmax(1)))\n",
        "    print(\"Target matrix: \",metrics.confusion_matrix(yt_test.argmax(1), yt_test_pred.argmax(1)))\n",
        "\n",
        "\n",
        "    N = np.arange(0,len(acc_source))\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, np.array(acc_source), label=\"Source accuracy\")\n",
        "    plt.plot(N, np.array(acc_target), label=\"Target accuracy\")\n",
        "    plt.title(\"Training Accuracy of Source and Target \")\n",
        "    plt.xlabel(\"Number of iteration\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6M1ewVXb1w5",
        "colab_type": "code",
        "outputId": "c9afffee-e9e2-4b24-fd53-3bf9f0a10e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "train_classifier(param)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 7, 7, 512)\n",
            "iter: 00099: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 55.78947\n",
            "iter: 00199: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 54.21053\n",
            "iter: 00299: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 52.63158\n",
            "iter: 00399: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 53.68421\n",
            "iter: 00499: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 51.57895\n",
            "iter: 00599: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 50.52632\n",
            "iter: 00699: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 52.63158\n",
            "iter: 00799: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 50.52632\n",
            "iter: 00899: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 50.52632\n",
            "iter: 00999: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 53.68421\n",
            "Source matrix:  [[95  0]\n",
            " [ 0 95]]\n",
            "Target matrix:  [[ 9 85]\n",
            " [ 3 93]]\n",
            "source accuracy test 1.0\n",
            "target accuracy test 0.484375\n",
            "Source matrix:  [[32  0]\n",
            " [ 0 32]]\n",
            "Target matrix:  [[ 0 33]\n",
            " [ 0 31]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQH37cTxOFF",
        "colab_type": "text"
      },
      "source": [
        "# Discriminative Method "
      ]
    }
  ]
}