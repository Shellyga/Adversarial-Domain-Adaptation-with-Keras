{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shelly Adversarial-Domain-Adaptation-with-Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shellyga/Adversarial-Domain-Adaptation-with-Keras/blob/master/Shelly_Adversarial_Domain_Adaptation_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL528F1bqeYW",
        "colab_type": "code",
        "outputId": "3ffa6034-2b21-4dc4-ba51-e4d0ac9b90b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPpk2rxrreIC",
        "colab_type": "code",
        "outputId": "a046e979-1dbd-4c97-8e1e-044a9496606d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQgQF0CqiOf",
        "colab_type": "text"
      },
      "source": [
        "# Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_egWQHVLqkNe",
        "colab_type": "code",
        "outputId": "e2dd787d-c1b9-40fe-ad36-5f853ea00efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "SEED = 7\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "# import tensorflow.python.keras as tf\n",
        "# from tensorflow.compat.v1 import set_random_seed\n",
        "# import tensorflow.python.keras as tf\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "# import model\n",
        "# import optimizer\n",
        "\n",
        "def pil_loader(path):\n",
        "    # print(path)\n",
        "    # Return the RGB variant of input image\n",
        "    with open(path, 'rb') as f:\n",
        "      with Image.open(f) as img:\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def one_hot_encoding(param):\n",
        "    # Read the source and target labels from param\n",
        "    s_label = param[\"source_label\"]\n",
        "    t_label = param[\"target_label\"]\n",
        "\n",
        "    # Encode the labels into one-hot format\n",
        "    classes = (np.concatenate((s_label, t_label), axis = 0))\n",
        "    num_classes = np.max(classes)\n",
        "    if 0 in classes:\n",
        "            num_classes = num_classes+1\n",
        "    s_label = to_categorical(s_label, num_classes = num_classes)\n",
        "    t_label = to_categorical(t_label, num_classes = num_classes)\n",
        "    return s_label, t_label\n",
        "            \n",
        "def data_loader(filepath, inp_dims):\n",
        "    # Load images and corresponding labels from the text file, stack them in numpy arrays and return\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
        "        # sys.exit() \n",
        "    img = []\n",
        "    label = []\n",
        "    with open(filepath,'r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "            token = line.split()\n",
        "            # print('drive/My Drive/project_data/'+token[0])\n",
        "            image_path = \"drive/My Drive/project_data/\"+token[0]\n",
        "            i = pil_loader(image_path)\n",
        "            i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "            img.append(np.array(i))\n",
        "            label.append(int(token[1]))\n",
        "    img = np.array(img)\n",
        "    label = np.array(label)\n",
        "    return img, label\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    #Generate batches of data.\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr\n",
        "\n",
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp)\n",
        "    classifier = build_classifier(param, embedding)\n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "\n",
        "    if param[\"number_of_gpus\"] > 1:\n",
        "        models[\"combined_classifier\"] = multi_gpu_model(build_combined_classifier(inp, classifier), gpus = param[\"number_of_gpus\"])\n",
        "        models[\"combined_discriminator\"] = multi_gpu_model(build_combined_discriminator(inp, discriminator), gpus = param[\"number_of_gpus\"])\n",
        "        models[\"combined_model\"] = multi_gpu_model(build_combined_model(inp, [classifier, discriminator]), gpus = param[\"number_of_gpus\"])\n",
        "    else:\n",
        "        models[\"combined_classifier\"] = build_combined_classifier(inp, classifier)\n",
        "        models[\"combined_discriminator\"] = build_combined_discriminator(inp, discriminator)\n",
        "        models[\"combined_model\"] = build_combined_model(inp, [classifier, discriminator])\n",
        "\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = {'class_act_last': 'categorical_crossentropy', 'dis_act_last': \\\n",
        "        'binary_crossentropy'}, loss_weights = {'class_act_last': param[\"class_loss_weight\"], 'dis_act_last': param[\"dis_loss_weight\"]}, metrics = ['accuracy'])\n",
        "\n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "        adv_weights = []\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (layer.name.startswith(\"dis_\")):\n",
        "                adv_weights.append(layer.get_weights())\n",
        "          \n",
        "        stats1 = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1],\\\n",
        "                                sample_weight=[weight_class, weight_adv])            \n",
        "        k = 0\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (layer.name.startswith(\"dis_\")):                    \n",
        "                layer.set_weights(adv_weights[k])\n",
        "                k += 1\n",
        "\n",
        "        class_weights = []        \n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (not layer.name.startswith(\"dis_\")):\n",
        "                class_weights.append(layer.get_weights())  \n",
        "\n",
        "        stats2 = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "        k = 0\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (not layer.name.startswith(\"dis_\")):\n",
        "                layer.set_weights(class_weights[k])\n",
        "                k += 1\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt)\n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                # optim[\"labels\"] = ys_pred.argmax(1)\n",
        "\n",
        "                if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "                    gap_last_snap = 0\n",
        "                    np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "                    open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "                    models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        gap_last_snap = gap_last_snap + 1;\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read parameter values from the console\n",
        "    parser = argparse.ArgumentParser(description = 'Domain Adaptation')\n",
        "    parser.add_argument('--number_of_gpus', type = int, nargs = '?', default = '1', help = \"Number of gpus to run\")\n",
        "    parser.add_argument('--network_name', type = str, default = 'ResNet50', help = \"Name of the feature extractor network\")\n",
        "    parser.add_argument('--dataset_name', type = str, default = 'Office', help = \"Name of the source dataset\")\n",
        "    parser.add_argument('--dropout_classifier', type = float, default = 0.25, help = \"Dropout ratio for classifier\")\n",
        "    parser.add_argument('--dropout_discriminator', type = float, default = 0.25, help = \"Dropout ratio for discriminator\")    \n",
        "    parser.add_argument('--source_path', type = str, default = 'amazon_10_list.txt', help = \"Path to source dataset\")\n",
        "    parser.add_argument('--target_path', type = str, default = 'webcam_10_list.txt', help = \"Path to target dataset\")\n",
        "    parser.add_argument('--lr_classifier', type = float, default = 0.0001, help = \"Learning rate for classifier model\")\n",
        "    parser.add_argument('--b1_classifier', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for classifier model optimizer\")\n",
        "    parser.add_argument('--b2_classifier', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for classifier model optimizer\")\n",
        "    parser.add_argument('--lr_discriminator', type = float, default = 0.00001, help = \"Learning rate for discriminator model\")\n",
        "    parser.add_argument('--b1_discriminator', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for discriminator model optimizer\")\n",
        "    parser.add_argument('--b2_discriminator', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for discriminator model optimizer\")\n",
        "    parser.add_argument('--lr_combined', type = float, default = 0.00001, help = \"Learning rate for combined model\")\n",
        "    parser.add_argument('--b1_combined', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for combined model optimizer\")\n",
        "    parser.add_argument('--b2_combined', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for combined model optimizer\")\n",
        "    parser.add_argument('--classifier_loss_weight', type = float, default = 1, help = \"Classifier loss weight\")\n",
        "    parser.add_argument('--discriminator_loss_weight', type = float, default = 4, help = \"Discriminator loss weight\")\n",
        "    parser.add_argument('--batch_size', type = int, default = 32, help = \"Batch size for training\")\n",
        "    parser.add_argument('--test_interval', type = int, default = 3, help = \"Gap between two successive test phases\")\n",
        "    parser.add_argument('--num_iterations', type = int, default = 12000, help = \"Number of iterations\")\n",
        "    parser.add_argument('--snapshot_interval', type = int, default = 500, help = \"Minimum gap between saving outputs\")\n",
        "    parser.add_argument('--output_dir', type = str, default = 'Models', help = \"Directory for saving outputs\")\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Set GPU device\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(list(np.arange(args.number_of_gpus))).strip('[]')\n",
        "\n",
        "    # Initialize parameters\n",
        "    param = {}\n",
        "    param[\"number_of_gpus\"] = 1\n",
        "    param[\"network_name\"] = 'ResNet50'\n",
        "    param[\"inp_dims\"] = [224, 224, 3]\n",
        "    # param[\"num_iterations\"] = 12000\n",
        "    param[\"num_iterations\"] = 120\n",
        "    param[\"lr_classifier\"] = 0.0001\n",
        "    param[\"b1_classifier\"] = 0.9\n",
        "    param[\"b2_classifier\"] = 0.999    \n",
        "    param[\"lr_discriminator\"] = 0.00001\n",
        "    param[\"b1_discriminator\"] =  0.9\n",
        "    param[\"b2_discriminator\"] = 0.999\n",
        "    param[\"lr_combined\"] = 0.00001\n",
        "    param[\"b1_combined\"] =  0.9\n",
        "    param[\"b2_combined\"] =  0.999       \n",
        "    param[\"batch_size\"] = int(32/2)\n",
        "    param[\"class_loss_weight\"] = 1\n",
        "    param[\"dis_loss_weight\"] = 4    \n",
        "    param[\"drop_classifier\"] = 0.25\n",
        "    param[\"drop_discriminator\"] = 0.25\n",
        "    param[\"test_interval\"] = 3\n",
        "    param[\"source_path\"] = 'drive/My Drive/project_data/your_file.txt'\n",
        "    param[\"target_path\"] = 'drive/My Drive/project_data/your_file_shelly.txt' \n",
        "    # param[\"snapshot_interval\"] = 500\n",
        "    param[\"snapshot_interval\"] = 5\n",
        "    param[\"output_path\"] = 'drive/My Drive/project_data/result'\n",
        "\n",
        "    # Create directory for saving models and log files\n",
        "    if not os.path.exists(param[\"output_path\"]):\n",
        "        os.mkdir(param[\"output_path\"])\n",
        "    \n",
        "    # Load source and target data\n",
        "    param[\"source_data\"], param[\"source_label\"] = data_loader(param[\"source_path\"], param[\"inp_dims\"])\n",
        "    param[\"target_data\"], param[\"target_label\"] = data_loader(param[\"target_path\"], param[\"inp_dims\"])\n",
        "\n",
        "    # Encode labels into one-hot format\n",
        "    param[\"source_label\"], param[\"target_label\"] = one_hot_encoding(param)\n",
        "\n",
        "    # Train data\n",
        "    train(param)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "0.8583333333333333\n",
            "0.5583333333333333\n",
            "iter: 00002: \n",
            "LABEL CLASSIFICATION: source_accuracy: 65.00000, target_accuracy: 53.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 85.83333, target_domain_accuracy: 55.83333 \n",
            "\n",
            "0.9333333333333333\n",
            "0.825\n",
            "iter: 00005: \n",
            "LABEL CLASSIFICATION: source_accuracy: 80.00000, target_accuracy: 63.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 93.33333, target_domain_accuracy: 82.50000 \n",
            "\n",
            "0.9666666666666667\n",
            "0.9416666666666667\n",
            "iter: 00008: \n",
            "LABEL CLASSIFICATION: source_accuracy: 89.16667, target_accuracy: 64.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 96.66667, target_domain_accuracy: 94.16667 \n",
            "\n",
            "0.9916666666666667\n",
            "0.95\n",
            "iter: 00011: \n",
            "LABEL CLASSIFICATION: source_accuracy: 95.00000, target_accuracy: 61.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 95.00000 \n",
            "\n",
            "0.9916666666666667\n",
            "0.9833333333333333\n",
            "iter: 00014: \n",
            "LABEL CLASSIFICATION: source_accuracy: 98.33333, target_accuracy: 59.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 98.33333 \n",
            "\n",
            "0.9916666666666667\n",
            "0.9833333333333333\n",
            "iter: 00017: \n",
            "LABEL CLASSIFICATION: source_accuracy: 97.50000, target_accuracy: 58.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 98.33333 \n",
            "\n",
            "0.9916666666666667\n",
            "0.9833333333333333\n",
            "iter: 00020: \n",
            "LABEL CLASSIFICATION: source_accuracy: 97.50000, target_accuracy: 60.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 98.33333 \n",
            "\n",
            "0.9916666666666667\n",
            "0.9916666666666667\n",
            "iter: 00023: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 59.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 99.16667 \n",
            "\n",
            "0.9916666666666667\n",
            "1.0\n",
            "iter: 00026: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 59.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 100.00000 \n",
            "\n",
            "0.9916666666666667\n",
            "1.0\n",
            "iter: 00029: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 60.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 100.00000 \n",
            "\n",
            "0.9916666666666667\n",
            "1.0\n",
            "iter: 00032: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 61.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00035: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 64.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00038: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 63.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00041: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00044: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00047: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00050: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00053: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00056: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00059: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 66.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00062: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 66.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00065: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00068: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00071: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00074: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00077: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00080: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00083: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00086: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00089: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00092: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00095: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00098: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00101: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00104: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00107: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00110: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 69.16667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00113: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00116: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "1.0\n",
            "1.0\n",
            "iter: 00119: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkLUpK4yxW1-",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8pb9VyIqh_q",
        "colab_type": "text"
      },
      "source": [
        "# Traib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBL2_iTDqmX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp)\n",
        "    \n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "    models[\"combined_discriminator\"] = build_combined_discriminator(inp, discriminator)     \n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "   \n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "        adv_weights = []  \n",
        "\n",
        "        stats2 = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "\n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "            print(source_domain_accuracy)\n",
        "            print(target_domain_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2tAkuZEvWDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('drive/My Drive/project_data/your_file_shelly.txt','r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "          token = line.split()\n",
        "          print(token[0])\n",
        "          image_path = \"drive/My Drive/project_data/\"+token[0]\n",
        "          # print(image_path)\n",
        "          with open(image_path, 'rb') as f:\n",
        "              with Image.open(f) as img:\n",
        "                  image= img.convert('RGB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqYXgaotqk_g",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxcy4fLWqpgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization, Activation, Dropout\n",
        "\n",
        "def build_embedding(param, inp):\n",
        "    network = eval(param[\"network_name\"])\n",
        "    base = network(weights = 'imagenet', include_top = False)\n",
        "    feat = base(inp)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "def build_classifier(param, embedding):\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "\n",
        "    densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return actl\n",
        "\n",
        "def build_discriminator(param, embedding):\n",
        "    dense1 = Dense(400, name = 'dis_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name='dis_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'dis_act1')(bn1)\n",
        "    drop1 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'dis_dense2')(drop1)\n",
        "    bn2 = BatchNormalization(name='dis_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'dis_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop2')(act2)\n",
        "\n",
        "    densel = Dense(1, name = 'dis_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'dis_bn_last')(densel)\n",
        "    actl = Activation('sigmoid', name = 'dis_act_last')(bnl)\n",
        "    return actl\n",
        "\n",
        "def build_combined_classifier(inp, classifier):\n",
        "    comb_model = Model(inputs = inp, outputs = [classifier])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_discriminator(inp, discriminator):\n",
        "    comb_model = Model(inputs = inp, outputs = [discriminator])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_model(inp, comb):\n",
        "    comb_model = Model(inputs = inp, outputs = comb)\n",
        "    return comb_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DrTIu1SqtIe",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8K7kUmqu5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def opt_classifier(param):\n",
        "    return Adam(lr=param[\"lr_classifier\"], beta_1=param[\"b1_classifier\"], beta_2=param[\"b2_classifier\"])\n",
        "\n",
        "def opt_discriminator(param):\n",
        "    return Adam(lr=param[\"lr_discriminator\"], beta_1=param[\"b1_discriminator\"], beta_2=param[\"b2_discriminator\"])\n",
        "\n",
        "def opt_combined(param):\n",
        "    return Adam(lr=param[\"lr_combined\"], beta_1=param[\"b1_combined\"], beta_2=param[\"b2_combined\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHRxvLjYqwvG",
        "colab_type": "text"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlgU5w6qyJO",
        "colab_type": "code",
        "outputId": "31a2f36d-0483-4c95-fb72-49c9eb4ace4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}