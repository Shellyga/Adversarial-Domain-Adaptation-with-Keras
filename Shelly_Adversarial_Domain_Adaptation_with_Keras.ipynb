{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shelly Adversarial-Domain-Adaptation-with-Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shellyga/Adversarial-Domain-Adaptation-with-Keras/blob/master/Shelly_Adversarial_Domain_Adaptation_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL528F1bqeYW",
        "colab_type": "code",
        "outputId": "ef77ab10-5d40-4a7b-f4e7-2cd7768207ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPpk2rxrreIC",
        "colab_type": "code",
        "outputId": "c1ea01f6-d9e7-4be8-dd9f-dbf4f3399778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQgQF0CqiOf",
        "colab_type": "text"
      },
      "source": [
        "# Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_egWQHVLqkNe",
        "colab_type": "code",
        "outputId": "f4ab3ee0-3c16-4f85-aa37-ae30fa3169af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "SEED = 7\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "# import tensorflow.python.keras as tf\n",
        "from tensorflow.compat.v1 import set_random_seed\n",
        "# import tensorflow.python.keras as tf\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "np.random.seed(SEED)\n",
        "set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "from PIL import Image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "# import model\n",
        "# import optimizer\n",
        "\n",
        "def pil_loader(path):\n",
        "    # print(path)\n",
        "    # Return the RGB variant of input image\n",
        "    with open(path, 'rb') as f:\n",
        "      with Image.open(f) as img:\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def one_hot_encoding(param):\n",
        "    # Read the source and target labels from param\n",
        "    s_label = param[\"source_label\"]\n",
        "    t_label = param[\"target_label\"]\n",
        "\n",
        "    # Encode the labels into one-hot format\n",
        "    classes = (np.concatenate((s_label, t_label), axis = 0))\n",
        "    num_classes = np.max(classes)\n",
        "    if 0 in classes:\n",
        "            num_classes = num_classes+1\n",
        "    s_label = to_categorical(s_label, num_classes = num_classes)\n",
        "    t_label = to_categorical(t_label, num_classes = num_classes)\n",
        "    return s_label, t_label\n",
        "            \n",
        "def data_loader(filepath, inp_dims):\n",
        "    # Load images and corresponding labels from the text file, stack them in numpy arrays and return\n",
        "    if not os.path.isfile(filepath):\n",
        "        print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
        "        # sys.exit() \n",
        "    img = []\n",
        "    label = []\n",
        "    with open(filepath,'r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "            token = line.split()\n",
        "            # print('drive/My Drive/project_data/'+token[0])\n",
        "            image_path = \"drive/My Drive/project_data/\"+token[0]\n",
        "            i = pil_loader(image_path)\n",
        "            i = i.resize((inp_dims[0], inp_dims[1]), Image.ANTIALIAS)\n",
        "            img.append(np.array(i))\n",
        "            label.append(int(token[1]))\n",
        "    img = np.array(img)\n",
        "    label = np.array(label)\n",
        "    return img, label\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    #Generate batches of data.\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size = batch_size, replace = False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr\n",
        "\n",
        "def train(param):\n",
        "    models = {}\n",
        "    inp = Input(shape = (param[\"inp_dims\"]))\n",
        "    embedding = build_embedding(param, inp)\n",
        "    classifier = build_classifier(param, embedding)\n",
        "    # classifier = build_classifier(param)\n",
        "    discriminator = build_discriminator(param, embedding)\n",
        "\n",
        "    if param[\"number_of_gpus\"] > 1:\n",
        "        models[\"combined_classifier\"] = multi_gpu_model(build_combined_classifier(inp, classifier), gpus = param[\"number_of_gpus\"])\n",
        "        models[\"combined_discriminator\"] = multi_gpu_model(build_combined_discriminator(inp, discriminator), gpus = param[\"number_of_gpus\"])\n",
        "        models[\"combined_model\"] = multi_gpu_model(build_combined_model(inp, [classifier, discriminator]), gpus = param[\"number_of_gpus\"])\n",
        "    else:\n",
        "        models[\"combined_classifier\"] = build_combined_classifier(inp, classifier)\n",
        "        models[\"combined_discriminator\"] = build_combined_discriminator(inp, discriminator)\n",
        "        models[\"combined_model\"] = build_combined_model(inp, [classifier, discriminator])\n",
        "\n",
        "    models[\"combined_classifier\"].compile(optimizer = opt_classifier(param), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    models[\"combined_model\"].compile(optimizer = opt_combined(param), loss = {'class_act_last': 'categorical_crossentropy', 'dis_act_last': \\\n",
        "        'binary_crossentropy'}, loss_weights = {'class_act_last': param[\"class_loss_weight\"], 'dis_act_last': param[\"dis_loss_weight\"]}, metrics = ['accuracy'])\n",
        "     \n",
        "    Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "    Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "    # Xs_train, Xs_test, Ys_train, Ys_test = train_test_split(Xs, ys, test_size=0.25, random_state=0)\n",
        "    # Xt_train, Xt_test, Yt_train, Yt_test = train_test_split(Xt, yt, test_size=0.25, random_state=0)\n",
        "\n",
        "    # Source domain is represented by label 0 and Target by 1\n",
        "    ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "    yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "    y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "    y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "    weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "    weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "    S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "    param[\"target_accuracy\"] = 0\n",
        "\n",
        "    optim = {}\n",
        "    optim[\"iter\"] = 0\n",
        "    optim[\"acc\"] = \"\"\n",
        "    optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "    gap_last_snap = 0\n",
        "\n",
        "    for i in range(param[\"num_iterations\"]):        \n",
        "        Xsb, ysb = next(S_batches)\n",
        "        Xtb, ytb = next(T_batches)\n",
        "        X_adv = np.concatenate([Xsb, Xtb])\n",
        "        y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "        adv_weights = []\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (layer.name.startswith(\"dis_\")):\n",
        "                adv_weights.append(layer.get_weights())\n",
        "          \n",
        "        stats1 = models[\"combined_model\"].train_on_batch(X_adv, [y_class, y_advb_1],\\\n",
        "                                sample_weight=[weight_class, weight_adv])            \n",
        "        k = 0\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (layer.name.startswith(\"dis_\")):                    \n",
        "                layer.set_weights(adv_weights[k])\n",
        "                k += 1\n",
        "\n",
        "        class_weights = []        \n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (not layer.name.startswith(\"dis_\")):\n",
        "                class_weights.append(layer.get_weights())  \n",
        "\n",
        "        stats2 = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "        k = 0\n",
        "        for layer in models[\"combined_model\"].layers:\n",
        "            if (not layer.name.startswith(\"dis_\")):\n",
        "                layer.set_weights(class_weights[k])\n",
        "                k += 1\n",
        "\n",
        "        if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "            ys_pred = models[\"combined_classifier\"].predict(Xs)\n",
        "            yt_pred = models[\"combined_classifier\"].predict(Xt)\n",
        "            ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "            yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "\n",
        "            source_accuracy = accuracy_score(ys.argmax(1), ys_pred.argmax(1))              \n",
        "            target_accuracy = accuracy_score(yt.argmax(1), yt_pred.argmax(1))\n",
        "            source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "            target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "            log_str = \"iter: {:05d}: \\nLABEL CLASSIFICATION: source_accuracy: {:.5f}, target_accuracy: {:.5f}\\\n",
        "                    \\nDOMAIN DISCRIMINATION: source_domain_accuracy: {:.5f}, target_domain_accuracy: {:.5f} \\n\"\\\n",
        "                                                         .format(i, source_accuracy*100, target_accuracy*100,\n",
        "                                                      source_domain_accuracy*100, target_domain_accuracy*100)\n",
        "            print(log_str)\n",
        "\n",
        "            if param[\"target_accuracy\"] < target_accuracy:              \n",
        "                optim[\"iter\"] = i\n",
        "                optim[\"acc\"] = log_str\n",
        "                # optim[\"labels\"] = ys_pred.argmax(1)\n",
        "\n",
        "                if (gap_last_snap >= param[\"snapshot_interval\"]):\n",
        "                    gap_last_snap = 0\n",
        "                    np.save(os.path.join(param[\"output_path\"],\"yPred_{}\".format(optim[\"iter\"])), optim[\"labels\"])\n",
        "                    open(os.path.join(param[\"output_path\"], \"acc_{}.txt\".format(optim[\"iter\"])), \"w\").write(optim[\"acc\"])\n",
        "                    models[\"combined_classifier\"].save(os.path.join(param[\"output_path\"],\"iter_{:05d}_model.h5\".format(i)))\n",
        "        gap_last_snap = gap_last_snap + 1;\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read parameter values from the console\n",
        "    parser = argparse.ArgumentParser(description = 'Domain Adaptation')\n",
        "    parser.add_argument('--number_of_gpus', type = int, nargs = '?', default = '1', help = \"Number of gpus to run\")\n",
        "    parser.add_argument('--network_name', type = str, default = 'ResNet50', help = \"Name of the feature extractor network\")\n",
        "    parser.add_argument('--dataset_name', type = str, default = 'Office', help = \"Name of the source dataset\")\n",
        "    parser.add_argument('--dropout_classifier', type = float, default = 0.25, help = \"Dropout ratio for classifier\")\n",
        "    parser.add_argument('--dropout_discriminator', type = float, default = 0.25, help = \"Dropout ratio for discriminator\")    \n",
        "    parser.add_argument('--source_path', type = str, default = 'amazon_10_list.txt', help = \"Path to source dataset\")\n",
        "    parser.add_argument('--target_path', type = str, default = 'webcam_10_list.txt', help = \"Path to target dataset\")\n",
        "    parser.add_argument('--lr_classifier', type = float, default = 0.0001, help = \"Learning rate for classifier model\")\n",
        "    parser.add_argument('--b1_classifier', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for classifier model optimizer\")\n",
        "    parser.add_argument('--b2_classifier', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for classifier model optimizer\")\n",
        "    parser.add_argument('--lr_discriminator', type = float, default = 0.00001, help = \"Learning rate for discriminator model\")\n",
        "    parser.add_argument('--b1_discriminator', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for discriminator model optimizer\")\n",
        "    parser.add_argument('--b2_discriminator', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for discriminator model optimizer\")\n",
        "    parser.add_argument('--lr_combined', type = float, default = 0.00001, help = \"Learning rate for combined model\")\n",
        "    parser.add_argument('--b1_combined', type = float, default = 0.9, help = \"Exponential decay rate of first moment \\\n",
        "                                                                                             for combined model optimizer\")\n",
        "    parser.add_argument('--b2_combined', type = float, default = 0.999, help = \"Exponential decay rate of second moment \\\n",
        "                                                                                            for combined model optimizer\")\n",
        "    parser.add_argument('--classifier_loss_weight', type = float, default = 1, help = \"Classifier loss weight\")\n",
        "    parser.add_argument('--discriminator_loss_weight', type = float, default = 4, help = \"Discriminator loss weight\")\n",
        "    parser.add_argument('--batch_size', type = int, default = 32, help = \"Batch size for training\")\n",
        "    parser.add_argument('--test_interval', type = int, default = 3, help = \"Gap between two successive test phases\")\n",
        "    parser.add_argument('--num_iterations', type = int, default = 12000, help = \"Number of iterations\")\n",
        "    parser.add_argument('--snapshot_interval', type = int, default = 500, help = \"Minimum gap between saving outputs\")\n",
        "    parser.add_argument('--output_dir', type = str, default = 'Models', help = \"Directory for saving outputs\")\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Set GPU device\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(list(np.arange(args.number_of_gpus))).strip('[]')\n",
        "\n",
        "    # Initialize parameters\n",
        "    param = {}\n",
        "    param[\"number_of_gpus\"] = 1\n",
        "    param[\"network_name\"] = 'ResNet50'\n",
        "    param[\"inp_dims\"] = [224, 224, 3]\n",
        "    # param[\"num_iterations\"] = 12000\n",
        "    param[\"num_iterations\"] = 500\n",
        "    param[\"lr_classifier\"] = 0.0001\n",
        "    param[\"b1_classifier\"] = 0.9\n",
        "    param[\"b2_classifier\"] = 0.999    \n",
        "    param[\"lr_discriminator\"] = 0.00001\n",
        "    param[\"b1_discriminator\"] =  0.9\n",
        "    param[\"b2_discriminator\"] = 0.999\n",
        "    param[\"lr_combined\"] = 0.00001\n",
        "    param[\"b1_combined\"] =  0.9\n",
        "    param[\"b2_combined\"] =  0.999       \n",
        "    param[\"batch_size\"] = int(32/2)\n",
        "    param[\"class_loss_weight\"] = 1\n",
        "    param[\"dis_loss_weight\"] = 4    \n",
        "    param[\"drop_classifier\"] = 0.25\n",
        "    param[\"drop_discriminator\"] = 0.25\n",
        "    param[\"test_interval\"] = 3\n",
        "    param[\"source_path\"] = 'drive/My Drive/project_data/your_file.txt'\n",
        "    param[\"target_path\"] = 'drive/My Drive/project_data/your_file_shelly.txt' \n",
        "    # param[\"snapshot_interval\"] = 500\n",
        "    param[\"snapshot_interval\"] = 5\n",
        "    param[\"output_path\"] = 'drive/My Drive/project_data/result2'\n",
        "\n",
        "    # Create directory for saving models and log files\n",
        "    if not os.path.exists(param[\"output_path\"]):\n",
        "        os.mkdir(param[\"output_path\"])\n",
        "    \n",
        "    # Load source and target data\n",
        "    param[\"source_data\"], param[\"source_label\"] = data_loader(param[\"source_path\"], param[\"inp_dims\"])\n",
        "    param[\"target_data\"], param[\"target_label\"] = data_loader(param[\"target_path\"], param[\"inp_dims\"])\n",
        "\n",
        "    # Encode labels into one-hot format\n",
        "    param[\"source_label\"], param[\"target_label\"] = one_hot_encoding(param)\n",
        "\n",
        "    # Train data\n",
        "    train(param)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n",
            "iter: 00002: \n",
            "LABEL CLASSIFICATION: source_accuracy: 42.50000, target_accuracy: 60.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 80.83333, target_domain_accuracy: 40.00000 \n",
            "\n",
            "iter: 00005: \n",
            "LABEL CLASSIFICATION: source_accuracy: 45.00000, target_accuracy: 63.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 93.33333, target_domain_accuracy: 68.33333 \n",
            "\n",
            "iter: 00008: \n",
            "LABEL CLASSIFICATION: source_accuracy: 49.16667, target_accuracy: 65.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 97.50000, target_domain_accuracy: 83.33333 \n",
            "\n",
            "iter: 00011: \n",
            "LABEL CLASSIFICATION: source_accuracy: 55.00000, target_accuracy: 65.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 98.33333, target_domain_accuracy: 88.33333 \n",
            "\n",
            "iter: 00014: \n",
            "LABEL CLASSIFICATION: source_accuracy: 62.50000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 98.33333, target_domain_accuracy: 95.00000 \n",
            "\n",
            "iter: 00017: \n",
            "LABEL CLASSIFICATION: source_accuracy: 70.83333, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 98.33333, target_domain_accuracy: 96.66667 \n",
            "\n",
            "iter: 00020: \n",
            "LABEL CLASSIFICATION: source_accuracy: 77.50000, target_accuracy: 66.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 98.33333, target_domain_accuracy: 98.33333 \n",
            "\n",
            "iter: 00023: \n",
            "LABEL CLASSIFICATION: source_accuracy: 81.66667, target_accuracy: 65.00000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 98.33333, target_domain_accuracy: 99.16667 \n",
            "\n",
            "iter: 00026: \n",
            "LABEL CLASSIFICATION: source_accuracy: 85.00000, target_accuracy: 65.83333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00029: \n",
            "LABEL CLASSIFICATION: source_accuracy: 90.00000, target_accuracy: 66.66667                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 99.16667, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00032: \n",
            "LABEL CLASSIFICATION: source_accuracy: 92.50000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00035: \n",
            "LABEL CLASSIFICATION: source_accuracy: 93.33333, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00038: \n",
            "LABEL CLASSIFICATION: source_accuracy: 95.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00041: \n",
            "LABEL CLASSIFICATION: source_accuracy: 96.66667, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00044: \n",
            "LABEL CLASSIFICATION: source_accuracy: 97.50000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00047: \n",
            "LABEL CLASSIFICATION: source_accuracy: 98.33333, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00050: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00053: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00056: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00059: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00062: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00065: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00068: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00071: \n",
            "LABEL CLASSIFICATION: source_accuracy: 99.16667, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00074: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00077: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00080: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00083: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00086: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00089: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00092: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00095: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00098: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00101: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00104: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00107: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00110: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00113: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00116: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00119: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00122: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00125: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00128: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00131: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00134: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00137: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00140: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00143: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00146: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00149: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00152: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00155: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00158: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00161: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00164: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00167: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00170: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00173: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00176: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00179: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 67.50000                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00182: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00185: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00188: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00191: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00194: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00197: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00200: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00203: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00206: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00209: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00212: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00215: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00218: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00221: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00224: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00227: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00230: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00233: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00236: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00239: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00242: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00245: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n",
            "iter: 00248: \n",
            "LABEL CLASSIFICATION: source_accuracy: 100.00000, target_accuracy: 68.33333                    \n",
            "DOMAIN DISCRIMINATION: source_domain_accuracy: 100.00000, target_domain_accuracy: 100.00000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8pb9VyIqh_q",
        "colab_type": "text"
      },
      "source": [
        "# Traib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBL2_iTDqmX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train(param):\n",
        "#     models = {}\n",
        "#     inp = Input(shape = (param[\"inp_dims\"]))\n",
        "#     embedding = build_embedding(param, inp)\n",
        "    \n",
        "#     discriminator = build_discriminator(param, embedding)\n",
        "#     models[\"combined_discriminator\"] = build_combined_discriminator(inp, discriminator)     \n",
        "#     models[\"combined_discriminator\"].compile(optimizer = opt_discriminator(param), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "   \n",
        "#     Xs, ys = param[\"source_data\"], param[\"source_label\"]\n",
        "#     Xt, yt = param[\"target_data\"], param[\"target_label\"]\n",
        "\n",
        "#     # Source domain is represented by label 0 and Target by 1\n",
        "#     ys_adv = np.array(([0.] * ys.shape[0]))\n",
        "#     yt_adv = np.array(([1.] * yt.shape[0]))\n",
        "\n",
        "#     y_advb_1 = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"])) # For gradient reversal\n",
        "#     y_advb_2 = np.array(([0] * param[\"batch_size\"] + [1] * param[\"batch_size\"]))\n",
        "#     weight_class = np.array(([1] * param[\"batch_size\"] + [0] * param[\"batch_size\"]))\n",
        "#     weight_adv = np.ones((param[\"batch_size\"] * 2,))\n",
        "#     S_batches = batch_generator([Xs, ys], param[\"batch_size\"])\n",
        "#     T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),))], param[\"batch_size\"])\n",
        "\n",
        "#     param[\"target_accuracy\"] = 0\n",
        "\n",
        "#     optim = {}\n",
        "#     optim[\"iter\"] = 0\n",
        "#     optim[\"acc\"] = \"\"\n",
        "#     optim[\"labels\"] = np.array(Xt.shape[0],)\n",
        "#     gap_last_snap = 0\n",
        "\n",
        "#     for i in range(param[\"num_iterations\"]):        \n",
        "#         Xsb, ysb = next(S_batches)\n",
        "#         Xtb, ytb = next(T_batches)\n",
        "#         X_adv = np.concatenate([Xsb, Xtb])\n",
        "#         y_class = np.concatenate([ysb, np.zeros_like(ysb)])\n",
        "\n",
        "#         adv_weights = []  \n",
        "\n",
        "#         stats2 = models[\"combined_discriminator\"].train_on_batch(X_adv, [y_advb_2])\n",
        "\n",
        "#         if ((i + 1) % param[\"test_interval\"] == 0):\n",
        "\n",
        "#             ys_adv_pred = models[\"combined_discriminator\"].predict(Xs)\n",
        "#             yt_adv_pred = models[\"combined_discriminator\"].predict(Xt)\n",
        "#             source_domain_accuracy = accuracy_score(ys_adv, np.round(ys_adv_pred))              \n",
        "#             target_domain_accuracy = accuracy_score(yt_adv, np.round(yt_adv_pred))\n",
        "#             print(source_domain_accuracy)\n",
        "#             print(target_domain_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2tAkuZEvWDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('drive/My Drive/project_data/your_file_shelly.txt','r',encoding='utf-8-sig') as fp:\n",
        "        for line in fp:\n",
        "          token = line.split()\n",
        "          print(token[0])\n",
        "          image_path = \"drive/My Drive/project_data/\"+token[0]\n",
        "          # print(image_path)\n",
        "          with open(image_path, 'rb') as f:\n",
        "              with Image.open(f) as img:\n",
        "                  image= img.convert('RGB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqYXgaotqk_g",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxcy4fLWqpgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization, Activation, Dropout\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_embedding(param, inp):\n",
        "    network = eval(param[\"network_name\"])\n",
        "    base = network(weights = 'imagenet', include_top = False)\n",
        "    feat = base(inp)\n",
        "    flat = Flatten()(feat)\n",
        "    return flat\n",
        "\n",
        "def build_classifier(param, embedding):\n",
        "    dense1 = Dense(400, name = 'class_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name = 'class_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'class_act1')(bn1)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'class_dense2')(drop2)\n",
        "    bn2 = BatchNormalization(name = 'class_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'class_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_classifier\"], name = 'class_drop2')(act2)\n",
        "\n",
        "    densel = Dense(param[\"source_label\"].shape[1], name = 'class_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'class_bn_last')(densel)\n",
        "    actl = Activation('softmax', name = 'class_act_last')(bnl)\n",
        "    return actl\n",
        "\n",
        "def build_discriminator(param, embedding):\n",
        "    dense1 = Dense(400, name = 'dis_dense1')(embedding)\n",
        "    bn1 = BatchNormalization(name='dis_bn1')(dense1)\n",
        "    act1 = Activation('relu', name = 'dis_act1')(bn1)\n",
        "    drop1 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop1')(act1)\n",
        "\n",
        "    dense2 = Dense(100, name = 'dis_dense2')(drop1)\n",
        "    bn2 = BatchNormalization(name='dis_bn2')(dense2)\n",
        "    act2 = Activation('relu', name = 'dis_act2')(bn2)\n",
        "    drop2 = Dropout(param[\"drop_discriminator\"], name = 'dis_drop2')(act2)\n",
        "\n",
        "    densel = Dense(1, name = 'dis_dense_last')(drop2)\n",
        "    bnl = BatchNormalization(name = 'dis_bn_last')(densel)\n",
        "    actl = Activation('sigmoid', name = 'dis_act_last')(bnl)\n",
        "    return actl\n",
        "\n",
        "def build_combined_classifier(inp, classifier):\n",
        "    comb_model = Model(inputs = inp, outputs = [classifier])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_discriminator(inp, discriminator):\n",
        "    comb_model = Model(inputs = inp, outputs = [discriminator])\n",
        "    return comb_model\n",
        "\n",
        "def build_combined_model(inp, comb):\n",
        "    comb_model = Model(inputs = inp, outputs = comb)\n",
        "    return comb_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57CQpjm7OklS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install keras_vggface"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DrTIu1SqtIe",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8K7kUmqu5W",
        "colab_type": "code",
        "outputId": "8f248405-603d-4a8f-ffa0-8a1eaf6c98a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def opt_classifier(param):\n",
        "    return Adam(lr=param[\"lr_classifier\"], beta_1=param[\"b1_classifier\"], beta_2=param[\"b2_classifier\"])\n",
        "\n",
        "def opt_discriminator(param):\n",
        "    return Adam(lr=param[\"lr_discriminator\"], beta_1=param[\"b1_discriminator\"], beta_2=param[\"b2_discriminator\"])\n",
        "\n",
        "def opt_combined(param):\n",
        "    return Adam(lr=param[\"lr_combined\"], beta_1=param[\"b1_combined\"], beta_2=param[\"b2_combined\"])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHRxvLjYqwvG",
        "colab_type": "text"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxlgU5w6qyJO",
        "colab_type": "code",
        "outputId": "d12c6def-3a07-4448-e855-bad61dcb339c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}